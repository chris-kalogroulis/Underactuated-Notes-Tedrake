{"cells":[{"block_group":"b4b295f270f04ec68003870f55207421","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"3f9d13a0f2f743c5807b780646ffd4bc","deepnote_block_group":"b4b295f270f04ec68003870f55207421","deepnote_cell_type":"markdown","deepnote_sorting_key":"0","deepnote_source":"This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/lqr.html).  I recommend having both windows open, side-by-side!\n"},"source":"This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/lqr.html).  I recommend having both windows open, side-by-side!\n"},{"block_group":"0fdd9f823e4248a5b6bcae0344ab89bf","cell_type":"code","execution_count":null,"metadata":{"cell_id":"82635b2052e14a8686192fbd732b0da0","deepnote_block_group":"0fdd9f823e4248a5b6bcae0344ab89bf","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_source":"import numpy as np\nfrom IPython.display import clear_output\nfrom pydrake.all import DiscreteAlgebraicRiccatiEquation\n\nfrom underactuated import running_as_notebook"},"outputs":[],"source":"import numpy as np\nfrom IPython.display import clear_output\nfrom pydrake.all import DiscreteAlgebraicRiccatiEquation\n\nfrom underactuated import running_as_notebook"},{"block_group":"3fe0efb70a05425296e4664168f8b0b0","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"48a0fc74a7fc4d01aaef6e38a1d8b20d","deepnote_block_group":"3fe0efb70a05425296e4664168f8b0b0","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"# LQR via Fitted Value Iteration\n\nDouble integrator.  Discrete-time, infinite-horizon, discounted.  This is the \"traditional\" fitted value iteration, $J=x^TSx$ as the (linear) function approximator.  It samples both $x$ and $u$, and takes an argmin over the samples $u$."},"source":"# LQR via Fitted Value Iteration\n\nDouble integrator.  Discrete-time, infinite-horizon, discounted.  This is the \"traditional\" fitted value iteration, $J=x^TSx$ as the (linear) function approximator.  It samples both $x$ and $u$, and takes an argmin over the samples $u$."},{"block_group":"197962a80e32422a97969ce1c5bc5196","cell_type":"code","execution_count":null,"metadata":{"cell_id":"c09edb96e058436ab3b554d3041d6669","deepnote_block_group":"197962a80e32422a97969ce1c5bc5196","deepnote_cell_type":"code","deepnote_sorting_key":"3","deepnote_source":"# Define the double integrator\ntime_step = 0.1\nA = np.eye(2) + time_step * np.array([[0.0, 1.0], [0.0, 0.0]])\nB = time_step * np.array([[0.0], [1.0]])\nQ = time_step * np.eye(2)\nR = time_step * np.eye(1)\n\n\ndef quadratic_regulator_cost(x, u):\n    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n\n\ndef FittedValueIteration(S_guess, gamma=0.9):\n    S_optimal = DiscreteAlgebraicRiccatiEquation(\n        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n    )\n\n    x1s = np.linspace(-5, 5, 31)\n    x2s = np.linspace(-4, 4, 31)\n    us = np.linspace(-1, 1, 9)\n    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing=\"ij\")\n    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n    UwithX = Us.flatten().reshape(1, -1)\n    Nx = x1s.size * x2s.size\n    X = XwithU[:, :Nx]\n    N = X1s.size\n\n    Xnext = A @ XwithU + B @ UwithX\n    Cost = quadratic_regulator_cost(XwithU, UwithX)\n    Jnext = np.zeros((1, N))\n    Jd = np.zeros((1, Nx))\n\n    def cost_to_go(S, X):\n        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n\n    def mean_bellman_residual(S, X, J_desired):\n        N = J_desired.size\n        err = cost_to_go(S, X) - J_desired\n        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n        dloss_dS = (\n            2\n            / N\n            * X\n            @ np.diag(\n                err.reshape(\n                    -1,\n                )\n            )\n            @ X.T\n        )\n        return loss, dloss_dS\n\n    S = S_guess\n    eta = 0.0001\n    last_loss = np.inf\n    for epoch in range(1000 if running_as_notebook else 2):\n        Jnext = cost_to_go(S, Xnext)\n        Jd[:] = np.min((Cost + gamma * Jnext).reshape(us.size, Nx), axis=0)\n        for i in range(10):\n            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n            S -= eta * dloss_dS\n        if epoch % 10 == 0:\n            clear_output(wait=True)\n            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n        if np.linalg.norm(last_loss - loss) < 1e-5:\n            break\n        last_loss = loss\n\n    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n    print(f\"optimal S= {S_optimal.flatten()}\")\n\n\nFittedValueIteration(S_guess=np.eye(2), gamma=0.9)"},"outputs":[],"source":"# Define the double integrator\ntime_step = 0.1\nA = np.eye(2) + time_step * np.array([[0.0, 1.0], [0.0, 0.0]])\nB = time_step * np.array([[0.0], [1.0]])\nQ = time_step * np.eye(2)\nR = time_step * np.eye(1)\n\n\ndef quadratic_regulator_cost(x, u):\n    return (x * (Q @ x)).sum(axis=0) + (u * (R @ u)).sum(axis=0)\n\n\ndef FittedValueIteration(S_guess, gamma=0.9):\n    S_optimal = DiscreteAlgebraicRiccatiEquation(\n        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n    )\n\n    x1s = np.linspace(-5, 5, 31)\n    x2s = np.linspace(-4, 4, 31)\n    us = np.linspace(-1, 1, 9)\n    Us, X1s, X2s = np.meshgrid(us, x1s, x2s, indexing=\"ij\")\n    XwithU = np.vstack((X1s.flatten(), X2s.flatten()))\n    UwithX = Us.flatten().reshape(1, -1)\n    Nx = x1s.size * x2s.size\n    X = XwithU[:, :Nx]\n    N = X1s.size\n\n    Xnext = A @ XwithU + B @ UwithX\n    Cost = quadratic_regulator_cost(XwithU, UwithX)\n    Jnext = np.zeros((1, N))\n    Jd = np.zeros((1, Nx))\n\n    def cost_to_go(S, X):\n        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n\n    def mean_bellman_residual(S, X, J_desired):\n        N = J_desired.size\n        err = cost_to_go(S, X) - J_desired\n        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n        dloss_dS = (\n            2\n            / N\n            * X\n            @ np.diag(\n                err.reshape(\n                    -1,\n                )\n            )\n            @ X.T\n        )\n        return loss, dloss_dS\n\n    S = S_guess\n    eta = 0.0001\n    last_loss = np.inf\n    for epoch in range(1000 if running_as_notebook else 2):\n        Jnext = cost_to_go(S, Xnext)\n        Jd[:] = np.min((Cost + gamma * Jnext).reshape(us.size, Nx), axis=0)\n        for i in range(10):\n            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n            S -= eta * dloss_dS\n        if epoch % 10 == 0:\n            clear_output(wait=True)\n            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n        if np.linalg.norm(last_loss - loss) < 1e-5:\n            break\n        last_loss = loss\n\n    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n    print(f\"optimal S= {S_optimal.flatten()}\")\n\n\nFittedValueIteration(S_guess=np.eye(2), gamma=0.9)"},{"block_group":"5d8e6b7aa2454577bd064f084ec2118e","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"df827e020de54a18b43dbefd0ec55f22","deepnote_block_group":"5d8e6b7aa2454577bd064f084ec2118e","deepnote_cell_type":"markdown","deepnote_sorting_key":"4","deepnote_source":"With the initial guess of $S$ and the discount factor $\\gamma$ that I've used above, the algorithm converges nicely.  You'll see that it is close to the optimal $S$, but not exactly the optimal $S$.\n\nInterestingly, if you choose $\\gamma$ to be much closer to 1, the algorithm will diverge. At some point, the sampled values over $u$ cause problems -- the Bellman equation does not actually have a steady-state solution for the discretized $u$ version of the problem (the cost-to-go is infinite).\n\nTo convince you that this is indeed the problem, here is a version that solves analytically for the optimal $u$ (given the estimated $S$).  It draws samples only over $x$, and converges beautifully to the true optimum, even when $\\gamma=1$."},"source":"With the initial guess of $S$ and the discount factor $\\gamma$ that I've used above, the algorithm converges nicely.  You'll see that it is close to the optimal $S$, but not exactly the optimal $S$.\n\nInterestingly, if you choose $\\gamma$ to be much closer to 1, the algorithm will diverge. At some point, the sampled values over $u$ cause problems -- the Bellman equation does not actually have a steady-state solution for the discretized $u$ version of the problem (the cost-to-go is infinite).\n\nTo convince you that this is indeed the problem, here is a version that solves analytically for the optimal $u$ (given the estimated $S$).  It draws samples only over $x$, and converges beautifully to the true optimum, even when $\\gamma=1$."},{"block_group":"5603ee0ea24e4212a6420edf133d2aea","cell_type":"code","execution_count":null,"metadata":{"cell_id":"dac48033267f4eed9fb5b5423f795691","deepnote_block_group":"5603ee0ea24e4212a6420edf133d2aea","deepnote_cell_type":"code","deepnote_sorting_key":"5","deepnote_source":"def FittedValueIteration(S_guess, gamma=0.9):\n    S_optimal = DiscreteAlgebraicRiccatiEquation(\n        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n    )\n\n    x1s = np.linspace(-5, 5, 31)\n    x2s = np.linspace(-4, 4, 31)\n    X1s, X2s = np.meshgrid(x1s, x2s, indexing=\"ij\")\n    X = np.vstack((X1s.flatten(), X2s.flatten()))\n    N = X1s.size\n\n    Jd = np.zeros((1, N))\n\n    def cost_to_go(S, X):\n        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n\n    def policy(S, X):\n        return -gamma * np.linalg.inv(R + gamma * B.T @ S @ B) @ B.T @ S @ A @ X\n\n    def mean_bellman_residual(S, X, J_desired):\n        N = J_desired.size\n        err = cost_to_go(S, X) - J_desired\n        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n        dloss_dS = (\n            2\n            / N\n            * X\n            @ np.diag(\n                err.reshape(\n                    -1,\n                )\n            )\n            @ X.T\n        )\n        return loss, dloss_dS\n\n    S = S_guess\n    eta = 0.0001\n    last_loss = np.inf\n    for epoch in range(1000 if running_as_notebook else 2):\n        U = policy(S, X)\n        Xnext = A @ X + B @ U\n        G = quadratic_regulator_cost(X, U)\n        Jd = G + gamma * cost_to_go(S, Xnext)\n        for i in range(10):\n            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n            S -= eta * dloss_dS\n        if epoch % 10 == 0:\n            clear_output(wait=True)\n            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n        if np.linalg.norm(last_loss - loss) < 1e-5:\n            break\n        last_loss = loss\n\n    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n    print(f\"optimal S= {S_optimal.flatten()}\")\n    return S\n\n\nFittedValueIteration(S_guess=np.eye(2), gamma=1);"},"outputs":[],"source":"def FittedValueIteration(S_guess, gamma=0.9):\n    S_optimal = DiscreteAlgebraicRiccatiEquation(\n        A=np.sqrt(gamma) * A, B=B, Q=Q, R=R / gamma\n    )\n\n    x1s = np.linspace(-5, 5, 31)\n    x2s = np.linspace(-4, 4, 31)\n    X1s, X2s = np.meshgrid(x1s, x2s, indexing=\"ij\")\n    X = np.vstack((X1s.flatten(), X2s.flatten()))\n    N = X1s.size\n\n    Jd = np.zeros((1, N))\n\n    def cost_to_go(S, X):\n        return (X * (S @ X)).sum(axis=0)  # vectorized quadratic form\n\n    def policy(S, X):\n        return -gamma * np.linalg.inv(R + gamma * B.T @ S @ B) @ B.T @ S @ A @ X\n\n    def mean_bellman_residual(S, X, J_desired):\n        N = J_desired.size\n        err = cost_to_go(S, X) - J_desired\n        loss = np.mean(err**2)  # == 1/N ∑ᵢ [tr(Sxᵢxᵢ')-yᵢ]²\n        # dloss_dS = 2/N ∑ᵢ errᵢ*xᵢxᵢ' = 2/N X*Diag(err)*X'\n        dloss_dS = (\n            2\n            / N\n            * X\n            @ np.diag(\n                err.reshape(\n                    -1,\n                )\n            )\n            @ X.T\n        )\n        return loss, dloss_dS\n\n    S = S_guess\n    eta = 0.0001\n    last_loss = np.inf\n    for epoch in range(1000 if running_as_notebook else 2):\n        U = policy(S, X)\n        Xnext = A @ X + B @ U\n        G = quadratic_regulator_cost(X, U)\n        Jd = G + gamma * cost_to_go(S, Xnext)\n        for i in range(10):\n            loss, dloss_dS = mean_bellman_residual(S, X, Jd)\n            S -= eta * dloss_dS\n        if epoch % 10 == 0:\n            clear_output(wait=True)\n            print(f\"epoch {epoch}: loss = {loss}, S = {S.flatten()}\")\n        if np.linalg.norm(last_loss - loss) < 1e-5:\n            break\n        last_loss = loss\n\n    print(f\"eigenvalues of S: {np.linalg.eig(S)[0]}\")\n    print(f\"optimal S= {S_optimal.flatten()}\")\n    return S\n\n\nFittedValueIteration(S_guess=np.eye(2), gamma=1);"},{"block_group":"0f546ede56fa47d58c857c55c47b6ad1","cell_type":"code","execution_count":null,"metadata":{"cell_id":"3fb7649256fc4cbcbbeadd6a7f9cc3a8","deepnote_block_group":"0f546ede56fa47d58c857c55c47b6ad1","deepnote_cell_type":"code","deepnote_sorting_key":"6","deepnote_source":""},"outputs":[],"source":""}],
        "metadata": {"deepnote_notebook_id":"d3b585e301764a90a8f4649465a07002"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }