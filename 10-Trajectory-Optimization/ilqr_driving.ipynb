{"cells":[{"block_group":"577c35a417de4d4a82fa3056b51abd4e","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"98de072d44cf406396de487c8d9a3242","deepnote_block_group":"577c35a417de4d4a82fa3056b51abd4e","deepnote_cell_type":"markdown","deepnote_sorting_key":"0","deepnote_source":"# Iterative Linear Quadratic Regulator"},"source":"# Iterative Linear Quadratic Regulator"},{"block_group":"6aedf26f617a436d9dde62fccc0e677c","cell_type":"code","execution_count":null,"metadata":{"cell_id":"53f1c05f69f041508a65604a74715a03","deepnote_block_group":"6aedf26f617a436d9dde62fccc0e677c","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pydrake.symbolic as sym"},"outputs":[],"source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pydrake.symbolic as sym"},{"block_group":"c49ad1d9977345c881ee08305ef0327a","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"2ffd5bd5a37d49ca8eca9d255129e64b","deepnote_block_group":"c49ad1d9977345c881ee08305ef0327a","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"## Iterative Linear Quadratic Regulator Derivation\n\nIn this exercise we will derive the iterative Linear Quadratic Regulator (iLQR) solving the following optimization problem.\n\n$\\begin{aligned} \\min_{\\mathbf{u}[\\cdot]} \\quad & \\ell_f(\\mathbf{x}[N]) + \\sum_{n=0}^{N-1} \\ell(\\mathbf{x}[n],\\mathbf{u}[n]) \\\\ \\text{subject to} \\quad & \\mathbf{x}[n+1] = f(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\forall n\\in[0, N-1] \\\\ & \\mathbf{x}[0] = \\mathbf{x}_0\\end{aligned}$\n\nAfter completing this exercise you will be able to write your own MPC solver from scratch without any proprietary or third-party software (with the exception of auto-differentiation). You will derive all necessary equations yourself.\nWhile the iLQR algorithm will be capable of solving general model predictive control problems in the form described above, we will apply it to the control of a vehicle. \n\n### Vehicle Control Problem\nBefore we start the actual derivation of iLQR we will take a look at the vehicle dynamics and cost functions. The vehicle has the following continuous time dynamics and is controlled by longitudinal acceleration and steering velocity."},"source":"## Iterative Linear Quadratic Regulator Derivation\n\nIn this exercise we will derive the iterative Linear Quadratic Regulator (iLQR) solving the following optimization problem.\n\n$\\begin{aligned} \\min_{\\mathbf{u}[\\cdot]} \\quad & \\ell_f(\\mathbf{x}[N]) + \\sum_{n=0}^{N-1} \\ell(\\mathbf{x}[n],\\mathbf{u}[n]) \\\\ \\text{subject to} \\quad & \\mathbf{x}[n+1] = f(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\forall n\\in[0, N-1] \\\\ & \\mathbf{x}[0] = \\mathbf{x}_0\\end{aligned}$\n\nAfter completing this exercise you will be able to write your own MPC solver from scratch without any proprietary or third-party software (with the exception of auto-differentiation). You will derive all necessary equations yourself.\nWhile the iLQR algorithm will be capable of solving general model predictive control problems in the form described above, we will apply it to the control of a vehicle. \n\n### Vehicle Control Problem\nBefore we start the actual derivation of iLQR we will take a look at the vehicle dynamics and cost functions. The vehicle has the following continuous time dynamics and is controlled by longitudinal acceleration and steering velocity."},{"block_group":"e6592ec3002645c38396ecc2e097689f","cell_type":"code","execution_count":null,"metadata":{"cell_id":"e02b3d71bc8447619510344c1cc643de","deepnote_block_group":"e6592ec3002645c38396ecc2e097689f","deepnote_cell_type":"code","deepnote_sorting_key":"3","deepnote_source":"n_x = 5\nn_u = 2\n\n\ndef car_continuous_dynamics(x, u):\n    # x = [x position, y position, heading, speed, steering angle]\n    # u = [acceleration, steering velocity]\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    heading = x[2]\n    v = x[3]\n    steer = x[4]\n    x_d = np.array(\n        [v * m.cos(heading), v * m.sin(heading), v * m.tan(steer), u[0], u[1]]\n    )\n    return x_d"},"outputs":[],"source":"n_x = 5\nn_u = 2\n\n\ndef car_continuous_dynamics(x, u):\n    # x = [x position, y position, heading, speed, steering angle]\n    # u = [acceleration, steering velocity]\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    heading = x[2]\n    v = x[3]\n    steer = x[4]\n    x_d = np.array(\n        [v * m.cos(heading), v * m.sin(heading), v * m.tan(steer), u[0], u[1]]\n    )\n    return x_d"},{"block_group":"5ce3d43a2d7d4a58bf29368122863cd4","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"689d075660c443b5bfae9fd22bb27ae1","deepnote_block_group":"5ce3d43a2d7d4a58bf29368122863cd4","deepnote_cell_type":"markdown","deepnote_sorting_key":"4","deepnote_source":"Note that while the vehicle dynamics are in continuous time, our problem formulation is in discrete time. Define the general discrete time dynamics $\\bf f$ with a simple [Euler integrator](https://en.wikipedia.org/wiki/Euler_method) in the next cell."},"source":"Note that while the vehicle dynamics are in continuous time, our problem formulation is in discrete time. Define the general discrete time dynamics $\\bf f$ with a simple [Euler integrator](https://en.wikipedia.org/wiki/Euler_method) in the next cell."},{"block_group":"d4e0b7b3db264a7c9237badec80dd819","cell_type":"code","execution_count":null,"metadata":{"cell_id":"767342870eaf4d96ac13619fac9508be","deepnote_block_group":"d4e0b7b3db264a7c9237badec80dd819","deepnote_cell_type":"code","deepnote_sorting_key":"5","deepnote_source":"def discrete_dynamics(x, u):\n    dt = 0.1\n    # TODO: Fill in the Euler integrator below and return the next state\n    x_next = x\n    return x_next"},"outputs":[],"source":"def discrete_dynamics(x, u):\n    dt = 0.1\n    # TODO: Fill in the Euler integrator below and return the next state\n    x_next = x\n    return x_next"},{"block_group":"a42145e3d3ce4fa59225a8de1ce9dc84","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"1889c2c7f9c54a9fa019d13edd035056","deepnote_block_group":"a42145e3d3ce4fa59225a8de1ce9dc84","deepnote_cell_type":"markdown","deepnote_sorting_key":"6","deepnote_source":"Given an initial state $\\mathbf{x}_0$ and a guess of a control trajectory $\\mathbf{u}[0:N-1]$ we roll out the state trajectory $x[0:N]$ until the time horizon $N$. Please complete the rollout function."},"source":"Given an initial state $\\mathbf{x}_0$ and a guess of a control trajectory $\\mathbf{u}[0:N-1]$ we roll out the state trajectory $x[0:N]$ until the time horizon $N$. Please complete the rollout function."},{"block_group":"19510ce9284c4c7491b8aad934a4ea45","cell_type":"code","execution_count":null,"metadata":{"cell_id":"bf68a9826b7b42aea0eaedcd7b325774","deepnote_block_group":"19510ce9284c4c7491b8aad934a4ea45","deepnote_cell_type":"code","deepnote_sorting_key":"7","deepnote_source":"def rollout(x0, u_trj):\n    x_trj = np.zeros((u_trj.shape[0] + 1, x0.shape[0]))\n    # TODO: Define the rollout here and return the state trajectory x_trj: [N, number of states]\n    return x_trj\n\n\n# Debug your implementation with this example code\nN = 10\nx0 = np.array([1, 0, 0, 1, 0])\nu_trj = np.zeros((N - 1, n_u))\nx_trj = rollout(x0, u_trj)"},"outputs":[],"source":"def rollout(x0, u_trj):\n    x_trj = np.zeros((u_trj.shape[0] + 1, x0.shape[0]))\n    # TODO: Define the rollout here and return the state trajectory x_trj: [N, number of states]\n    return x_trj\n\n\n# Debug your implementation with this example code\nN = 10\nx0 = np.array([1, 0, 0, 1, 0])\nu_trj = np.zeros((N - 1, n_u))\nx_trj = rollout(x0, u_trj)"},{"block_group":"661e46d58f2f450798ca09156d07abef","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"a35a476908474c7b82ecefbd2613d1c6","deepnote_block_group":"661e46d58f2f450798ca09156d07abef","deepnote_cell_type":"markdown","deepnote_sorting_key":"8","deepnote_source":"We define the stage cost function $\\ell$ and final cost function $\\ell_f$. The goal of these cost functions is to drive the vehicle along a circle with radius $r$ around the origin with a desired speed."},"source":"We define the stage cost function $\\ell$ and final cost function $\\ell_f$. The goal of these cost functions is to drive the vehicle along a circle with radius $r$ around the origin with a desired speed."},{"block_group":"4780524e4210405da72134397c7bf58a","cell_type":"code","execution_count":null,"metadata":{"cell_id":"06bc0fbfbcf1464686b136d53574c2b4","deepnote_block_group":"4780524e4210405da72134397c7bf58a","deepnote_cell_type":"code","deepnote_sorting_key":"9","deepnote_source":"r = 2.0\nv_target = 2.0\neps = 1e-6  # The derivative of sqrt(x) at x=0 is undefined. Avoid by subtle smoothing\n\n\ndef cost_stage(x, u):\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    c_circle = (m.sqrt(x[0] ** 2 + x[1] ** 2 + eps) - r) ** 2\n    c_speed = (x[3] - v_target) ** 2\n    c_control = (u[0] ** 2 + u[1] ** 2) * 0.1\n    return c_circle + c_speed + c_control\n\n\ndef cost_final(x):\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    c_circle = (m.sqrt(x[0] ** 2 + x[1] ** 2 + eps) - r) ** 2\n    c_speed = (x[3] - v_target) ** 2\n    return c_circle + c_speed"},"outputs":[],"source":"r = 2.0\nv_target = 2.0\neps = 1e-6  # The derivative of sqrt(x) at x=0 is undefined. Avoid by subtle smoothing\n\n\ndef cost_stage(x, u):\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    c_circle = (m.sqrt(x[0] ** 2 + x[1] ** 2 + eps) - r) ** 2\n    c_speed = (x[3] - v_target) ** 2\n    c_control = (u[0] ** 2 + u[1] ** 2) * 0.1\n    return c_circle + c_speed + c_control\n\n\ndef cost_final(x):\n    m = sym if x.dtype == object else np  # Check type for autodiff\n    c_circle = (m.sqrt(x[0] ** 2 + x[1] ** 2 + eps) - r) ** 2\n    c_speed = (x[3] - v_target) ** 2\n    return c_circle + c_speed"},{"block_group":"948a8c47b12b4e48b18cf897b2aeec16","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"41a53e714bad42b99524ac2bd0fd0cad","deepnote_block_group":"948a8c47b12b4e48b18cf897b2aeec16","deepnote_cell_type":"markdown","deepnote_sorting_key":"10","deepnote_source":"Your next task is to write the total cost function of the state and control trajectory. This is simply the sum of all stages over the control horizon and the objective from general problem formulation above."},"source":"Your next task is to write the total cost function of the state and control trajectory. This is simply the sum of all stages over the control horizon and the objective from general problem formulation above."},{"block_group":"59a0e42d08324bed99f56e6eecb72eae","cell_type":"code","execution_count":null,"metadata":{"cell_id":"cd8d8bd3c8d14895a683289d02665cbf","deepnote_block_group":"59a0e42d08324bed99f56e6eecb72eae","deepnote_cell_type":"code","deepnote_sorting_key":"11","deepnote_source":"def cost_trj(x_trj, u_trj):\n    total = 0.0\n    # TODO: Sum up all costs\n    return total\n\n\n# Debug your code\ncost_trj(x_trj, u_trj)"},"outputs":[],"source":"def cost_trj(x_trj, u_trj):\n    total = 0.0\n    # TODO: Sum up all costs\n    return total\n\n\n# Debug your code\ncost_trj(x_trj, u_trj)"},{"block_group":"7d74f7ecc9ff4d15be8aa337e5ddbfb5","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"434512f3e2ce4103bcab1c157c025dc5","deepnote_block_group":"7d74f7ecc9ff4d15be8aa337e5ddbfb5","deepnote_cell_type":"markdown","deepnote_sorting_key":"12","deepnote_source":"### Bellman Recursion\n\nNow that we are warmed up, let's derive the actual algorithm. We start with the Bellman equation known from lecture defining optimality in a recursively backwards in time.\n\n$$\\begin{aligned} V(\\mathbf{x}[n]) = & \\min_{\\mathbf{u}[n]} \\quad \\ell(\\mathbf{x}[n], \\mathbf{u}[n])  + V(\\mathbf{x}[n+1]) \\\\ \\end{aligned}$$\n\nYou may have noticed that we neglected a couple of constraints of the original problem formulation. The fully equivalent formulation is \n\n$$\\begin{aligned} \\min_{\\mathbf{u}[n]} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\forall n\\in[0, N-1] \\\\ \\text{subject to} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]) = \\ell(\\mathbf{x}[n], \\mathbf{u}[n])  + V(\\mathbf{x}[n+1]) \\\\ & V(\\mathbf{x}[N]) =   \\ell_f(\\mathbf{x}[N]) \\\\ & \\mathbf{x}[n+1] = {\\bf      f}(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\\\ & \\mathbf{x}[0] = \\mathbf{x}_0 \\end{aligned}$$\n\nThe definition of a Q-function will become handy during the derivation of the algorithm.\n\nThe key idea of iLQR is simple: Approximate the dynamics linearly and the costs quadratically around a nominal trajectory. We will expand all terms of the Q-function accordingly and optimize the resulting quadratic equation for an optimal linear control law in closed form. We will see that by applying the Bellman equation recursively backwards in time, the value function remains a quadratic.\nThe linear and quadratic approximations are computed around the nominal state $\\bf \\bar{x} = \\bf x - \\delta \\bf x$ and the nominal control $\\bf \\bar{u} = \\bf u - \\delta \\bf u$. After applying the Bellman equation backwards in time from time $N$ to $0$ (the backward pass), we will update the nominal controls $\\bf \\bar{u}$ and states $\\bf \\bar{x}$ by applying the computed linear feedback law from the backward pass and rolling out the dynamics from the initial state $\\bf x_0$ to the final horizon $N$. Iterating between backwards and forwards pass optimizes the control problem.\n\n### Q-function Expansion\n\nLet's start by expanding all terms in the Q-function of the Bellman equation. The quadaratic cost function is\n\n$$\\begin{aligned} \\ell(\\mathbf{x}[n], \\mathbf{u}[n]) \\approx \\ell_n + \\begin{bmatrix}\\ell_{\\mathbf{x},n} \\\\  \\ell_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix}\\ell_{\\mathbf{xx},n} &  \\ell_{\\mathbf{ux},n}^T\\\\ \\ell_{\\mathbf{ux},n} & \\ell_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix},\\end{aligned}$$\n\nand the dynamics function is\n\n$$\\begin{aligned} x[n+1]= \\mathbf{f}(\\mathbf{x}[n], \\mathbf{u}[n]) \\approx \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} & \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}. \\end{aligned}$$\n\nHere, $\\ell = \\ell(\\bar{\\mathbf{x}}, \\bar{\\mathbf{u}})$ and $\\mathbf{f} = \\mathbf{f}(\\bar{\\mathbf{x}}, \\bar{\\mathbf{u}})$. $\\ell_\\mathbf{x}, \\ell_\\mathbf{u}, \\mathbf{f}_\\mathbf{x}, \\mathbf{f}_\\mathbf{u}$ are the gradients and Jacobians evaluated at $\\bar{\\mathbf{x}}$ and $\\bar{\\mathbf{u}}$. $\\ell_\\mathbf{xx}, \\ell_\\mathbf{ux}, \\ell_\\mathbf{uu}$ are the Hessians at $\\bar{\\mathbf{x}}$ and $\\bar{\\mathbf{u}}$. The expansion of the final cost follows analogously.\nThe code to evaluate all the derivative terms is:"},"source":"### Bellman Recursion\n\nNow that we are warmed up, let's derive the actual algorithm. We start with the Bellman equation known from lecture defining optimality in a recursively backwards in time.\n\n$$\\begin{aligned} V(\\mathbf{x}[n]) = & \\min_{\\mathbf{u}[n]} \\quad \\ell(\\mathbf{x}[n], \\mathbf{u}[n])  + V(\\mathbf{x}[n+1]) \\\\ \\end{aligned}$$\n\nYou may have noticed that we neglected a couple of constraints of the original problem formulation. The fully equivalent formulation is \n\n$$\\begin{aligned} \\min_{\\mathbf{u}[n]} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\forall n\\in[0, N-1] \\\\ \\text{subject to} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]) = \\ell(\\mathbf{x}[n], \\mathbf{u}[n])  + V(\\mathbf{x}[n+1]) \\\\ & V(\\mathbf{x}[N]) =   \\ell_f(\\mathbf{x}[N]) \\\\ & \\mathbf{x}[n+1] = {\\bf      f}(\\mathbf{x}[n], \\mathbf{u}[n]), \\quad \\\\ & \\mathbf{x}[0] = \\mathbf{x}_0 \\end{aligned}$$\n\nThe definition of a Q-function will become handy during the derivation of the algorithm.\n\nThe key idea of iLQR is simple: Approximate the dynamics linearly and the costs quadratically around a nominal trajectory. We will expand all terms of the Q-function accordingly and optimize the resulting quadratic equation for an optimal linear control law in closed form. We will see that by applying the Bellman equation recursively backwards in time, the value function remains a quadratic.\nThe linear and quadratic approximations are computed around the nominal state $\\bf \\bar{x} = \\bf x - \\delta \\bf x$ and the nominal control $\\bf \\bar{u} = \\bf u - \\delta \\bf u$. After applying the Bellman equation backwards in time from time $N$ to $0$ (the backward pass), we will update the nominal controls $\\bf \\bar{u}$ and states $\\bf \\bar{x}$ by applying the computed linear feedback law from the backward pass and rolling out the dynamics from the initial state $\\bf x_0$ to the final horizon $N$. Iterating between backwards and forwards pass optimizes the control problem.\n\n### Q-function Expansion\n\nLet's start by expanding all terms in the Q-function of the Bellman equation. The quadaratic cost function is\n\n$$\\begin{aligned} \\ell(\\mathbf{x}[n], \\mathbf{u}[n]) \\approx \\ell_n + \\begin{bmatrix}\\ell_{\\mathbf{x},n} \\\\  \\ell_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix}\\ell_{\\mathbf{xx},n} &  \\ell_{\\mathbf{ux},n}^T\\\\ \\ell_{\\mathbf{ux},n} & \\ell_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix},\\end{aligned}$$\n\nand the dynamics function is\n\n$$\\begin{aligned} x[n+1]= \\mathbf{f}(\\mathbf{x}[n], \\mathbf{u}[n]) \\approx \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} & \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}. \\end{aligned}$$\n\nHere, $\\ell = \\ell(\\bar{\\mathbf{x}}, \\bar{\\mathbf{u}})$ and $\\mathbf{f} = \\mathbf{f}(\\bar{\\mathbf{x}}, \\bar{\\mathbf{u}})$. $\\ell_\\mathbf{x}, \\ell_\\mathbf{u}, \\mathbf{f}_\\mathbf{x}, \\mathbf{f}_\\mathbf{u}$ are the gradients and Jacobians evaluated at $\\bar{\\mathbf{x}}$ and $\\bar{\\mathbf{u}}$. $\\ell_\\mathbf{xx}, \\ell_\\mathbf{ux}, \\ell_\\mathbf{uu}$ are the Hessians at $\\bar{\\mathbf{x}}$ and $\\bar{\\mathbf{u}}$. The expansion of the final cost follows analogously.\nThe code to evaluate all the derivative terms is:"},{"block_group":"b1767917176f4e569adc7aef91640ce5","cell_type":"code","execution_count":null,"metadata":{"cell_id":"928dbb551f784765a4d19621bab19499","deepnote_block_group":"b1767917176f4e569adc7aef91640ce5","deepnote_cell_type":"code","deepnote_sorting_key":"13","deepnote_source":"class derivatives:\n    def __init__(self, discrete_dynamics, cost_stage, cost_final, n_x, n_u):\n        self.x_sym = np.array([sym.Variable(\"x_{}\".format(i)) for i in range(n_x)])\n        self.u_sym = np.array([sym.Variable(\"u_{}\".format(i)) for i in range(n_u)])\n        x = self.x_sym\n        u = self.u_sym\n\n        l = cost_stage(x, u)\n        self.l_x = sym.Jacobian([l], x).ravel()\n        self.l_u = sym.Jacobian([l], u).ravel()\n        self.l_xx = sym.Jacobian(self.l_x, x)\n        self.l_ux = sym.Jacobian(self.l_u, x)\n        self.l_uu = sym.Jacobian(self.l_u, u)\n\n        l_final = cost_final(x)\n        self.l_final_x = sym.Jacobian([l_final], x).ravel()\n        self.l_final_xx = sym.Jacobian(self.l_final_x, x)\n\n        f = discrete_dynamics(x, u)\n        self.f_x = sym.Jacobian(f, x)\n        self.f_u = sym.Jacobian(f, u)\n\n    def stage(self, x, u):\n        env = {self.x_sym[i]: x[i] for i in range(x.shape[0])}\n        env.update({self.u_sym[i]: u[i] for i in range(u.shape[0])})\n\n        l_x = sym.Evaluate(self.l_x, env).ravel()\n        l_u = sym.Evaluate(self.l_u, env).ravel()\n        l_xx = sym.Evaluate(self.l_xx, env)\n        l_ux = sym.Evaluate(self.l_ux, env)\n        l_uu = sym.Evaluate(self.l_uu, env)\n\n        f_x = sym.Evaluate(self.f_x, env)\n        f_u = sym.Evaluate(self.f_u, env)\n\n        return l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u\n\n    def final(self, x):\n        env = {self.x_sym[i]: x[i] for i in range(x.shape[0])}\n\n        l_final_x = sym.Evaluate(self.l_final_x, env).ravel()\n        l_final_xx = sym.Evaluate(self.l_final_xx, env)\n\n        return l_final_x, l_final_xx\n\n\nderivs = derivatives(discrete_dynamics, cost_stage, cost_final, n_x, n_u)\n# Test the output:\n# x = np.array([0, 0, 0, 0, 0])\n# u = np.array([0, 0])\n# print(derivs.stage(x, u))\n# print(derivs.final(x))"},"outputs":[],"source":"class derivatives:\n    def __init__(self, discrete_dynamics, cost_stage, cost_final, n_x, n_u):\n        self.x_sym = np.array([sym.Variable(\"x_{}\".format(i)) for i in range(n_x)])\n        self.u_sym = np.array([sym.Variable(\"u_{}\".format(i)) for i in range(n_u)])\n        x = self.x_sym\n        u = self.u_sym\n\n        l = cost_stage(x, u)\n        self.l_x = sym.Jacobian([l], x).ravel()\n        self.l_u = sym.Jacobian([l], u).ravel()\n        self.l_xx = sym.Jacobian(self.l_x, x)\n        self.l_ux = sym.Jacobian(self.l_u, x)\n        self.l_uu = sym.Jacobian(self.l_u, u)\n\n        l_final = cost_final(x)\n        self.l_final_x = sym.Jacobian([l_final], x).ravel()\n        self.l_final_xx = sym.Jacobian(self.l_final_x, x)\n\n        f = discrete_dynamics(x, u)\n        self.f_x = sym.Jacobian(f, x)\n        self.f_u = sym.Jacobian(f, u)\n\n    def stage(self, x, u):\n        env = {self.x_sym[i]: x[i] for i in range(x.shape[0])}\n        env.update({self.u_sym[i]: u[i] for i in range(u.shape[0])})\n\n        l_x = sym.Evaluate(self.l_x, env).ravel()\n        l_u = sym.Evaluate(self.l_u, env).ravel()\n        l_xx = sym.Evaluate(self.l_xx, env)\n        l_ux = sym.Evaluate(self.l_ux, env)\n        l_uu = sym.Evaluate(self.l_uu, env)\n\n        f_x = sym.Evaluate(self.f_x, env)\n        f_u = sym.Evaluate(self.f_u, env)\n\n        return l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u\n\n    def final(self, x):\n        env = {self.x_sym[i]: x[i] for i in range(x.shape[0])}\n\n        l_final_x = sym.Evaluate(self.l_final_x, env).ravel()\n        l_final_xx = sym.Evaluate(self.l_final_xx, env)\n\n        return l_final_x, l_final_xx\n\n\nderivs = derivatives(discrete_dynamics, cost_stage, cost_final, n_x, n_u)\n# Test the output:\n# x = np.array([0, 0, 0, 0, 0])\n# u = np.array([0, 0])\n# print(derivs.stage(x, u))\n# print(derivs.final(x))"},{"block_group":"09e49a76ba624005b8c4ca65f178ff45","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"8a8ed983318f449caed6c26e5e76cc22","deepnote_block_group":"09e49a76ba624005b8c4ca65f178ff45","deepnote_cell_type":"markdown","deepnote_sorting_key":"14","deepnote_source":"Expanding the second term of the Q-function of the Bellman equation, i.e. the value function at the next state $\\mathbf{x}[n+1]$, to second order yields \n\n$$\\begin{aligned} V(\\mathbf{x}[n+1]) \\approx V_{n+1} + V_{\\mathbf{x},n+1}^T  \\delta \\mathbf{x}[n+1] + \\frac{1}{2}\\delta \\mathbf{x}[n+1]^T V_{\\mathbf{xx},n+1} \\delta \\mathbf{x}[n+1],\\end{aligned}$$\n\nwhere $\\delta \\mathbf{x}[n+1]$ is given by\n\n$$\\begin{aligned} \\delta \\mathbf{x}[n+1] & = \\mathbf{x}[n+1] - \\bar{\\mathbf{x}}[n+1] \\\\ & = \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} &  \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} -  \\bar{\\mathbf{x}}[n+1] \\\\ & = \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} & \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} -  \\mathbf{f}(\\bar{\\mathbf{x}}[n], \\bar{\\mathbf{u}}[n]) \\\\ & = \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} &  \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}   \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}. \\end{aligned}$$\n\nWe have now expanded all terms of the Bellman equation and can regroup them in the form of\n\n$$\\begin{aligned} Q(\\mathbf{x}[n], \\mathbf{u}[n]) & \\approx \\ell_n + \\begin{bmatrix}\\ell_{\\mathbf{x},n} \\\\  \\ell_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix}\\ell_{\\mathbf{xx},n} &  \\ell_{\\mathbf{ux},n}^T\\\\  \\ell_{\\mathbf{ux},n} & \\ell_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}, \\\\ & \\quad + V_{n+1} + V_{\\mathbf{x},n+1}^T  \\delta \\mathbf{x}[n+1] + \\frac{1}{2}\\delta \\mathbf{x}[n+1]^T V_{\\mathbf{xx},n+1} \\delta \\mathbf{x}[n+1], \\\\& = Q_n + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix} Q_{\\mathbf{xx},n} & Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}.\\end{aligned}$$\n\nFind $Q_{\\mathbf{x},n}$, $Q_{\\mathbf{u},n}$, $Q_{\\mathbf{xx},n}$, $Q_{\\mathbf{ux},n}$, $Q_{\\mathbf{uu},n}$ in terms of $\\ell$ and $\\textbf{f}$ and their expansions by collecitng coefficients in $(\\cdot)\\delta \\mathbf{x}[n]$, $(\\cdot)\\delta \\mathbf{u}[n]$, $1/2 \\delta \\mathbf{x}[n]^T (\\cdot) \\delta \\mathbf{x}[n]$, and similar. Write your results in the corresponding function below."},"source":"Expanding the second term of the Q-function of the Bellman equation, i.e. the value function at the next state $\\mathbf{x}[n+1]$, to second order yields \n\n$$\\begin{aligned} V(\\mathbf{x}[n+1]) \\approx V_{n+1} + V_{\\mathbf{x},n+1}^T  \\delta \\mathbf{x}[n+1] + \\frac{1}{2}\\delta \\mathbf{x}[n+1]^T V_{\\mathbf{xx},n+1} \\delta \\mathbf{x}[n+1],\\end{aligned}$$\n\nwhere $\\delta \\mathbf{x}[n+1]$ is given by\n\n$$\\begin{aligned} \\delta \\mathbf{x}[n+1] & = \\mathbf{x}[n+1] - \\bar{\\mathbf{x}}[n+1] \\\\ & = \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} &  \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} -  \\bar{\\mathbf{x}}[n+1] \\\\ & = \\mathbf{f}_n + \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} & \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} -  \\mathbf{f}(\\bar{\\mathbf{x}}[n], \\bar{\\mathbf{u}}[n]) \\\\ & = \\begin{bmatrix}\\mathbf{f}_{\\mathbf{x},n} &  \\mathbf{f}_{\\mathbf{u},n} \\end{bmatrix}   \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}. \\end{aligned}$$\n\nWe have now expanded all terms of the Bellman equation and can regroup them in the form of\n\n$$\\begin{aligned} Q(\\mathbf{x}[n], \\mathbf{u}[n]) & \\approx \\ell_n + \\begin{bmatrix}\\ell_{\\mathbf{x},n} \\\\  \\ell_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix}\\ell_{\\mathbf{xx},n} &  \\ell_{\\mathbf{ux},n}^T\\\\  \\ell_{\\mathbf{ux},n} & \\ell_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}, \\\\ & \\quad + V_{n+1} + V_{\\mathbf{x},n+1}^T  \\delta \\mathbf{x}[n+1] + \\frac{1}{2}\\delta \\mathbf{x}[n+1]^T V_{\\mathbf{xx},n+1} \\delta \\mathbf{x}[n+1], \\\\& = Q_n + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix} Q_{\\mathbf{xx},n} & Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix}.\\end{aligned}$$\n\nFind $Q_{\\mathbf{x},n}$, $Q_{\\mathbf{u},n}$, $Q_{\\mathbf{xx},n}$, $Q_{\\mathbf{ux},n}$, $Q_{\\mathbf{uu},n}$ in terms of $\\ell$ and $\\textbf{f}$ and their expansions by collecitng coefficients in $(\\cdot)\\delta \\mathbf{x}[n]$, $(\\cdot)\\delta \\mathbf{u}[n]$, $1/2 \\delta \\mathbf{x}[n]^T (\\cdot) \\delta \\mathbf{x}[n]$, and similar. Write your results in the corresponding function below."},{"block_group":"6b4f39b8a5604f5a84c41d3a05d54307","cell_type":"code","execution_count":null,"metadata":{"cell_id":"ed87dceb23d84d11b731a0a4d9662d6a","deepnote_block_group":"6b4f39b8a5604f5a84c41d3a05d54307","deepnote_cell_type":"code","deepnote_sorting_key":"15","deepnote_source":"def Q_terms(l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u, V_x, V_xx):\n    # TODO: Define the Q-terms here\n    Q_x = np.zeros(l_x.shape)\n    Q_u = np.zeros(l_u.shape)\n    Q_xx = np.zeros(l_xx.shape)\n    Q_ux = np.zeros(l_ux.shape)\n    Q_uu = np.zeros(l_uu.shape)\n    return Q_x, Q_u, Q_xx, Q_ux, Q_uu"},"outputs":[],"source":"def Q_terms(l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u, V_x, V_xx):\n    # TODO: Define the Q-terms here\n    Q_x = np.zeros(l_x.shape)\n    Q_u = np.zeros(l_u.shape)\n    Q_xx = np.zeros(l_xx.shape)\n    Q_ux = np.zeros(l_ux.shape)\n    Q_uu = np.zeros(l_uu.shape)\n    return Q_x, Q_u, Q_xx, Q_ux, Q_uu"},{"block_group":"a9d32c6563ac421c82448038d13ab456","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"2e7cd22a00f64daf8137ef34567a6353","deepnote_block_group":"a9d32c6563ac421c82448038d13ab456","deepnote_cell_type":"markdown","deepnote_sorting_key":"16","deepnote_source":"### Q-function Optimization and Optimal Linear Control Law\nAmazing! Now that we have the Q-function in quadratic form, we can optimize for the optimal control gains in closed form.\nThe original formulation, i.e. optimizing over $\\mathbf{u}[n]$, $$\\begin{aligned} \\min_{\\mathbf{u}[n]} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]),\\end{aligned}$$ is equivalent to optimzing over $\\delta \\mathbf{u}[n]$.\n\n$$\\begin{aligned} \\delta \\mathbf{u}[n]^* = {\\arg\\!\\min}_{\\delta \\mathbf{u}[n]} \\quad Q_n  + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix} Q_{\\mathbf{xx},n} &  Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} = k + K \\delta \\mathbf{x}[n]\\end{aligned}$$\n\nIt turns out that the optimal control is linear in $\\delta \\mathbf{x}[n]$.\nSolve the quadratic optimization analytically and derive equations for the feedforward gains $k$ and feedback gains $K$. Implement the function below. Hint: You do not need to compute $Q_\\mathbf{uu}^{-1}$ by hand."},"source":"### Q-function Optimization and Optimal Linear Control Law\nAmazing! Now that we have the Q-function in quadratic form, we can optimize for the optimal control gains in closed form.\nThe original formulation, i.e. optimizing over $\\mathbf{u}[n]$, $$\\begin{aligned} \\min_{\\mathbf{u}[n]} \\quad & Q(\\mathbf{x}[n], \\mathbf{u}[n]),\\end{aligned}$$ is equivalent to optimzing over $\\delta \\mathbf{u}[n]$.\n\n$$\\begin{aligned} \\delta \\mathbf{u}[n]^* = {\\arg\\!\\min}_{\\delta \\mathbf{u}[n]} \\quad Q_n  + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix} ^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} ^T \\begin{bmatrix} Q_{\\mathbf{xx},n} &  Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n] \\end{bmatrix} = k + K \\delta \\mathbf{x}[n]\\end{aligned}$$\n\nIt turns out that the optimal control is linear in $\\delta \\mathbf{x}[n]$.\nSolve the quadratic optimization analytically and derive equations for the feedforward gains $k$ and feedback gains $K$. Implement the function below. Hint: You do not need to compute $Q_\\mathbf{uu}^{-1}$ by hand."},{"block_group":"f53e9c930e1e4c31913ea29e09170a0a","cell_type":"code","execution_count":null,"metadata":{"cell_id":"7aac829cf69c4efcba85e41ac772dc4d","deepnote_block_group":"f53e9c930e1e4c31913ea29e09170a0a","deepnote_cell_type":"code","deepnote_sorting_key":"17","deepnote_source":"def gains(Q_uu, Q_u, Q_ux):\n    Q_uu_inv = np.linalg.inv(Q_uu)\n    # TOD: Implement the feedforward gain k and feedback gain K.\n    k = np.zeros(Q_u.shape)\n    K = np.zeros(Q_ux.shape)\n    return k, K"},"outputs":[],"source":"def gains(Q_uu, Q_u, Q_ux):\n    Q_uu_inv = np.linalg.inv(Q_uu)\n    # TOD: Implement the feedforward gain k and feedback gain K.\n    k = np.zeros(Q_u.shape)\n    K = np.zeros(Q_ux.shape)\n    return k, K"},{"block_group":"206112f98ee047e38fbc2896aad97821","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"c29b91aade8247f2a5c277d3027955b9","deepnote_block_group":"206112f98ee047e38fbc2896aad97821","deepnote_cell_type":"markdown","deepnote_sorting_key":"18","deepnote_source":"### Value Function Backward Update\nWe are almost done! We need to derive the backwards update equation for the value function. We simply plugin the optimal control $\\delta \\mathbf{u}[n]^* = k + K \\delta \\mathbf{x}[n]$ back into the Q-function which yields the value function\n\n$$\\begin{aligned} V(\\mathbf{x}[n]) \\approx V_{n} + V_{\\mathbf{x},n}^T  \\delta \\mathbf{x}[n] + \\frac{1}{2}\\delta \\mathbf{x}[n]^T V_{\\mathbf{xx},n} \\delta \\mathbf{x}[n] = Q_n + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix}^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix}^T \\begin{bmatrix} Q_{\\mathbf{xx},n} &  Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix}. \\end{aligned}$$\n\nCompare terms in $(\\cdot) \\delta \\mathbf{x}[n]$ and $ 1/2 \\delta \\mathbf{x}[n]^T (\\cdot)  \\delta \\mathbf{x}[n]$, find $V_{\\mathbf{x},n}$, and $V_{\\mathbf{xx},n}$ and implement the corresponding function below.\n\nIMPORTANT: Do not simplify the expression you obtain for $V_{x}$ and $V_{xx}$ by assuming that $k$ and $K$ have the form computed by the function `gains`."},"source":"### Value Function Backward Update\nWe are almost done! We need to derive the backwards update equation for the value function. We simply plugin the optimal control $\\delta \\mathbf{u}[n]^* = k + K \\delta \\mathbf{x}[n]$ back into the Q-function which yields the value function\n\n$$\\begin{aligned} V(\\mathbf{x}[n]) \\approx V_{n} + V_{\\mathbf{x},n}^T  \\delta \\mathbf{x}[n] + \\frac{1}{2}\\delta \\mathbf{x}[n]^T V_{\\mathbf{xx},n} \\delta \\mathbf{x}[n] = Q_n + \\begin{bmatrix} Q_{\\mathbf{x},n} \\\\  Q_{\\mathbf{u},n} \\end{bmatrix}^T  \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix}^T \\begin{bmatrix} Q_{\\mathbf{xx},n} &  Q_{\\mathbf{ux},n}^T\\\\  Q_{\\mathbf{ux},n} & Q_{\\mathbf{uu},n}\\end{bmatrix} \\begin{bmatrix} \\delta \\mathbf{x}[n] \\\\ \\delta \\mathbf{u}[n]^* \\end{bmatrix}. \\end{aligned}$$\n\nCompare terms in $(\\cdot) \\delta \\mathbf{x}[n]$ and $ 1/2 \\delta \\mathbf{x}[n]^T (\\cdot)  \\delta \\mathbf{x}[n]$, find $V_{\\mathbf{x},n}$, and $V_{\\mathbf{xx},n}$ and implement the corresponding function below.\n\nIMPORTANT: Do not simplify the expression you obtain for $V_{x}$ and $V_{xx}$ by assuming that $k$ and $K$ have the form computed by the function `gains`."},{"block_group":"eddaec448134420dae238ae6ad6c56c8","cell_type":"code","execution_count":null,"metadata":{"cell_id":"ef883fef122f44829d48df9898ef7119","deepnote_block_group":"eddaec448134420dae238ae6ad6c56c8","deepnote_cell_type":"code","deepnote_sorting_key":"19","deepnote_source":"def V_terms(Q_x, Q_u, Q_xx, Q_ux, Q_uu, K, k):\n    # TODO: Implement V_x and V_xx, hint: use the A.dot(B) function for matrix multiplcation.\n    V_x = np.zeros(Q_x.shape)\n    V_xx = np.zeros(Q_xx.shape)\n    return V_x, V_xx"},"outputs":[],"source":"def V_terms(Q_x, Q_u, Q_xx, Q_ux, Q_uu, K, k):\n    # TODO: Implement V_x and V_xx, hint: use the A.dot(B) function for matrix multiplcation.\n    V_x = np.zeros(Q_x.shape)\n    V_xx = np.zeros(Q_xx.shape)\n    return V_x, V_xx"},{"block_group":"df578da41af740c5ba87c209722049ba","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"827b04f635314fa9b10b1c6ba7246c50","deepnote_block_group":"df578da41af740c5ba87c209722049ba","deepnote_cell_type":"markdown","deepnote_sorting_key":"20","deepnote_source":"### Expected Cost Reduction\nWe can also estimate by how much we expect to reduce the cost by applying the optimal controls. Simply subtract the previous nominal Q-value ($\\delta \\mathbf{x}[n] = 0$ and $\\delta \\mathbf{u}[n]=0$) from the value function.  The result is implemented below and is a useful aid in checking how accurate the quadratic approximation is during convergence of iLQR and adapting stepsize and regularization."},"source":"### Expected Cost Reduction\nWe can also estimate by how much we expect to reduce the cost by applying the optimal controls. Simply subtract the previous nominal Q-value ($\\delta \\mathbf{x}[n] = 0$ and $\\delta \\mathbf{u}[n]=0$) from the value function.  The result is implemented below and is a useful aid in checking how accurate the quadratic approximation is during convergence of iLQR and adapting stepsize and regularization."},{"block_group":"a1419354cc184d47acb5d06c9979ae8f","cell_type":"code","execution_count":null,"metadata":{"cell_id":"dfa66eb9860c45e08caf11e8d864e07b","deepnote_block_group":"a1419354cc184d47acb5d06c9979ae8f","deepnote_cell_type":"code","deepnote_sorting_key":"21","deepnote_source":"def expected_cost_reduction(Q_u, Q_uu, k):\n    return -Q_u.T.dot(k) - 0.5 * k.T.dot(Q_uu.dot(k))"},"outputs":[],"source":"def expected_cost_reduction(Q_u, Q_uu, k):\n    return -Q_u.T.dot(k) - 0.5 * k.T.dot(Q_uu.dot(k))"},{"block_group":"112240c9e1e3468891b63afeb7ab3bbf","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"f39c111ec8b544c3b7efca0e7fc48808","deepnote_block_group":"112240c9e1e3468891b63afeb7ab3bbf","deepnote_cell_type":"markdown","deepnote_sorting_key":"22","deepnote_source":"### Forward Pass\nWe have now have all the ingredients to implement the forward pass and the backward pass of iLQR. In the forward pass, at each timestep the new updated control $\\mathbf{u}' =  \\bar{\\mathbf{u}} + k + K (x' - \\bar{\\mathbf{x}})$ is applied and the dynamis propagated based on the updated control. The nominal control and state trajectory $\\bar{\\mathbf{u}}, \\bar{\\mathbf{x}}$ with which we computed $k$ and $K$ are then updated and we receive a new set of state and control trajectories."},"source":"### Forward Pass\nWe have now have all the ingredients to implement the forward pass and the backward pass of iLQR. In the forward pass, at each timestep the new updated control $\\mathbf{u}' =  \\bar{\\mathbf{u}} + k + K (x' - \\bar{\\mathbf{x}})$ is applied and the dynamis propagated based on the updated control. The nominal control and state trajectory $\\bar{\\mathbf{u}}, \\bar{\\mathbf{x}}$ with which we computed $k$ and $K$ are then updated and we receive a new set of state and control trajectories."},{"block_group":"56d1dfae150141988e33b1258b2f9fdf","cell_type":"code","execution_count":null,"metadata":{"cell_id":"b55356af5cb54a5b97c5017070539264","deepnote_block_group":"56d1dfae150141988e33b1258b2f9fdf","deepnote_cell_type":"code","deepnote_sorting_key":"23","deepnote_source":"def forward_pass(x_trj, u_trj, k_trj, K_trj):\n    x_trj_new = np.zeros(x_trj.shape)\n    x_trj_new[0, :] = x_trj[0, :]\n    u_trj_new = np.zeros(u_trj.shape)\n    # TODO: Implement the forward pass here\n    #     for n in range(u_trj.shape[0]):\n    #         u_trj_new[n,:] = # Apply feedback law\n    #         x_trj_new[n+1,:] = # Apply dynamics\n    return x_trj_new, u_trj_new"},"outputs":[],"source":"def forward_pass(x_trj, u_trj, k_trj, K_trj):\n    x_trj_new = np.zeros(x_trj.shape)\n    x_trj_new[0, :] = x_trj[0, :]\n    u_trj_new = np.zeros(u_trj.shape)\n    # TODO: Implement the forward pass here\n    #     for n in range(u_trj.shape[0]):\n    #         u_trj_new[n,:] = # Apply feedback law\n    #         x_trj_new[n+1,:] = # Apply dynamics\n    return x_trj_new, u_trj_new"},{"block_group":"87e32bd9bcd546a4ac1a987f742b3158","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"bb6056e57b334dffa88a2f45e04c8383","deepnote_block_group":"87e32bd9bcd546a4ac1a987f742b3158","deepnote_cell_type":"markdown","deepnote_sorting_key":"24","deepnote_source":"### Backward Pass\nThe backward pass starts from the terminal boundary condition $V(\\mathbf{x}[N]) =   \\ell_f(\\mathbf{x}[N])$, such that $V_{\\mathbf{x},N} = \\ell_{\\mathbf{x},f}$ and $V_{\\mathbf{xx},N} = \\ell_{\\mathbf{xx},f}$. In the backwards loop terms for the Q-function at $n$ are computed based on the quadratic value function approximation at $n+1$ and the derivatives and hessians of dynamics and cost functions at $n$. To solve for the gains $k$ and $K$ an inversion of the matrix $Q_\\mathbf{uu}$ is necessary. To ensure invertability and to improve conditioning we add a diagonal matrix to $Q_\\mathbf{uu}$. This is equivalent to adding a quadratic penalty on the distance of the new control trajectory from the control trajectory of the previous iteration. The result is a smaller stepsize and more conservative convergence properties."},"source":"### Backward Pass\nThe backward pass starts from the terminal boundary condition $V(\\mathbf{x}[N]) =   \\ell_f(\\mathbf{x}[N])$, such that $V_{\\mathbf{x},N} = \\ell_{\\mathbf{x},f}$ and $V_{\\mathbf{xx},N} = \\ell_{\\mathbf{xx},f}$. In the backwards loop terms for the Q-function at $n$ are computed based on the quadratic value function approximation at $n+1$ and the derivatives and hessians of dynamics and cost functions at $n$. To solve for the gains $k$ and $K$ an inversion of the matrix $Q_\\mathbf{uu}$ is necessary. To ensure invertability and to improve conditioning we add a diagonal matrix to $Q_\\mathbf{uu}$. This is equivalent to adding a quadratic penalty on the distance of the new control trajectory from the control trajectory of the previous iteration. The result is a smaller stepsize and more conservative convergence properties."},{"block_group":"1712d233624b481eb69c3941199cf1cb","cell_type":"code","execution_count":null,"metadata":{"cell_id":"65c101254f00436eb7123c61754c7ff1","deepnote_block_group":"1712d233624b481eb69c3941199cf1cb","deepnote_cell_type":"code","deepnote_sorting_key":"25","deepnote_source":"def backward_pass(x_trj, u_trj, regu):\n    k_trj = np.zeros([u_trj.shape[0], u_trj.shape[1]])\n    K_trj = np.zeros([u_trj.shape[0], u_trj.shape[1], x_trj.shape[1]])\n    expected_cost_redu = 0\n    # TODO: Set terminal boundary condition here (V_x, V_xx)\n    V_x = np.zeros((x_trj.shape[1],))\n    V_xx = np.zeros((x_trj.shape[1], x_trj.shape[1]))\n    for n in range(u_trj.shape[0] - 1, -1, -1):\n        # TODO: First compute derivatives, then the Q-terms\n        # l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u =\n        # Q_x, Q_u, Q_xx, Q_ux, Q_uu =\n        Q_x = np.zeros((x_trj.shape[1],))\n        Q_u = np.zeros((u_trj.shape[1],))\n        Q_xx = np.zeros((x_trj.shape[1], x_trj.shape[1]))\n        Q_ux = np.zeros((u_trj.shape[1], x_trj.shape[1]))\n        Q_uu = np.zeros((u_trj.shape[1], u_trj.shape[1]))\n        # We add regularization to ensure that Q_uu is invertible and nicely conditioned\n        Q_uu_regu = Q_uu + np.eye(Q_uu.shape[0]) * regu\n        k, K = gains(Q_uu_regu, Q_u, Q_ux)\n        k_trj[n, :] = k\n        K_trj[n, :, :] = K\n        V_x, V_xx = V_terms(Q_x, Q_u, Q_xx, Q_ux, Q_uu, K, k)\n        expected_cost_redu += expected_cost_reduction(Q_u, Q_uu, k)\n    return k_trj, K_trj, expected_cost_redu"},"outputs":[],"source":"def backward_pass(x_trj, u_trj, regu):\n    k_trj = np.zeros([u_trj.shape[0], u_trj.shape[1]])\n    K_trj = np.zeros([u_trj.shape[0], u_trj.shape[1], x_trj.shape[1]])\n    expected_cost_redu = 0\n    # TODO: Set terminal boundary condition here (V_x, V_xx)\n    V_x = np.zeros((x_trj.shape[1],))\n    V_xx = np.zeros((x_trj.shape[1], x_trj.shape[1]))\n    for n in range(u_trj.shape[0] - 1, -1, -1):\n        # TODO: First compute derivatives, then the Q-terms\n        # l_x, l_u, l_xx, l_ux, l_uu, f_x, f_u =\n        # Q_x, Q_u, Q_xx, Q_ux, Q_uu =\n        Q_x = np.zeros((x_trj.shape[1],))\n        Q_u = np.zeros((u_trj.shape[1],))\n        Q_xx = np.zeros((x_trj.shape[1], x_trj.shape[1]))\n        Q_ux = np.zeros((u_trj.shape[1], x_trj.shape[1]))\n        Q_uu = np.zeros((u_trj.shape[1], u_trj.shape[1]))\n        # We add regularization to ensure that Q_uu is invertible and nicely conditioned\n        Q_uu_regu = Q_uu + np.eye(Q_uu.shape[0]) * regu\n        k, K = gains(Q_uu_regu, Q_u, Q_ux)\n        k_trj[n, :] = k\n        K_trj[n, :, :] = K\n        V_x, V_xx = V_terms(Q_x, Q_u, Q_xx, Q_ux, Q_uu, K, k)\n        expected_cost_redu += expected_cost_reduction(Q_u, Q_uu, k)\n    return k_trj, K_trj, expected_cost_redu"},{"block_group":"d68bf3e62b844526a0485d1adda78225","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"23bee6558c25495e827430dfd0988807","deepnote_block_group":"d68bf3e62b844526a0485d1adda78225","deepnote_cell_type":"markdown","deepnote_sorting_key":"26","deepnote_source":"### Main Loop\n\nThe main iLQR loop consists of iteratively applying the forward and backward pass. The regularization is adapted based on whether the new control and state trajectories improved the cost. We lower the regularization if the total cost was reduced and accept the new trajectory pair. If the total cost did not decrease, the trajectory pair is rejected and the regularization is increased. You may want to test the algorithm with deactivated regularization and observe the changed behavior.\nThe main loop stops if the maximum number of iterations is reached or the expected reduction is below a certain threshold.\n\nIf you have correctly implemented all subparts of the iLQR you should see that the car plans to drive around the circle."},"source":"### Main Loop\n\nThe main iLQR loop consists of iteratively applying the forward and backward pass. The regularization is adapted based on whether the new control and state trajectories improved the cost. We lower the regularization if the total cost was reduced and accept the new trajectory pair. If the total cost did not decrease, the trajectory pair is rejected and the regularization is increased. You may want to test the algorithm with deactivated regularization and observe the changed behavior.\nThe main loop stops if the maximum number of iterations is reached or the expected reduction is below a certain threshold.\n\nIf you have correctly implemented all subparts of the iLQR you should see that the car plans to drive around the circle."},{"block_group":"5ef526e40da14d55a588c933899a75f2","cell_type":"code","execution_count":null,"metadata":{"cell_id":"138ec9f966764603984397182690a733","deepnote_block_group":"5ef526e40da14d55a588c933899a75f2","deepnote_cell_type":"code","deepnote_sorting_key":"27","deepnote_source":"def run_ilqr(x0, N, max_iter=50, regu_init=100):\n    # First forward rollout\n    u_trj = np.random.randn(N - 1, n_u) * 0.0001\n    x_trj = rollout(x0, u_trj)\n    total_cost = cost_trj(x_trj, u_trj)\n    regu = regu_init\n    max_regu = 10000\n    min_regu = 0.01\n\n    # Setup traces\n    cost_trace = [total_cost]\n    expected_cost_redu_trace = []\n    redu_ratio_trace = [1]\n    redu_trace = []\n    regu_trace = [regu]\n\n    # Run main loop\n    for it in range(max_iter):\n        # Backward and forward pass\n        k_trj, K_trj, expected_cost_redu = backward_pass(x_trj, u_trj, regu)\n        x_trj_new, u_trj_new = forward_pass(x_trj, u_trj, k_trj, K_trj)\n        # Evaluate new trajectory\n        total_cost = cost_trj(x_trj_new, u_trj_new)\n        cost_redu = cost_trace[-1] - total_cost\n        redu_ratio = cost_redu / abs(expected_cost_redu)\n        # Accept or reject iteration\n        if cost_redu > 0:\n            # Improvement! Accept new trajectories and lower regularization\n            redu_ratio_trace.append(redu_ratio)\n            cost_trace.append(total_cost)\n            x_trj = x_trj_new\n            u_trj = u_trj_new\n            regu *= 0.7\n        else:\n            # Reject new trajectories and increase regularization\n            regu *= 2.0\n            cost_trace.append(cost_trace[-1])\n            redu_ratio_trace.append(0)\n        regu = min(max(regu, min_regu), max_regu)\n        regu_trace.append(regu)\n        redu_trace.append(cost_redu)\n\n        # Early termination if expected improvement is small\n        if expected_cost_redu <= 1e-6:\n            break\n\n    return x_trj, u_trj, cost_trace, regu_trace, redu_ratio_trace, redu_trace\n\n\n# Setup problem and call iLQR\nx0 = np.array([-3.0, 1.0, -0.2, 0.0, 0.0])\nN = 50\nmax_iter = 50\nregu_init = 100\nx_trj, u_trj, cost_trace, regu_trace, redu_ratio_trace, redu_trace = run_ilqr(\n    x0, N, max_iter, regu_init\n)\n\n\nplt.figure(figsize=(9.5, 8))\n# Plot circle\ntheta = np.linspace(0, 2 * np.pi, 100)\nplt.plot(r * np.cos(theta), r * np.sin(theta), linewidth=5)\nax = plt.gca()\n\n# Plot resulting trajecotry of car\nplt.plot(x_trj[:, 0], x_trj[:, 1], linewidth=5)\nw = 2.0\nh = 1.0\n\n# Plot rectangles\nfor n in range(x_trj.shape[0]):\n    rect = mpl.patches.Rectangle((-w / 2, -h / 2), w, h, fill=False)\n    t = (\n        mpl.transforms.Affine2D()\n        .rotate_deg_around(0, 0, np.rad2deg(x_trj[n, 2]))\n        .translate(x_trj[n, 0], x_trj[n, 1])\n        + ax.transData\n    )\n    rect.set_transform(t)\n    ax.add_patch(rect)\nax.set_aspect(1)\nplt.ylim((-3, 3))\nplt.xlim((-4.5, 3))\nplt.tight_layout()"},"outputs":[],"source":"def run_ilqr(x0, N, max_iter=50, regu_init=100):\n    # First forward rollout\n    u_trj = np.random.randn(N - 1, n_u) * 0.0001\n    x_trj = rollout(x0, u_trj)\n    total_cost = cost_trj(x_trj, u_trj)\n    regu = regu_init\n    max_regu = 10000\n    min_regu = 0.01\n\n    # Setup traces\n    cost_trace = [total_cost]\n    expected_cost_redu_trace = []\n    redu_ratio_trace = [1]\n    redu_trace = []\n    regu_trace = [regu]\n\n    # Run main loop\n    for it in range(max_iter):\n        # Backward and forward pass\n        k_trj, K_trj, expected_cost_redu = backward_pass(x_trj, u_trj, regu)\n        x_trj_new, u_trj_new = forward_pass(x_trj, u_trj, k_trj, K_trj)\n        # Evaluate new trajectory\n        total_cost = cost_trj(x_trj_new, u_trj_new)\n        cost_redu = cost_trace[-1] - total_cost\n        redu_ratio = cost_redu / abs(expected_cost_redu)\n        # Accept or reject iteration\n        if cost_redu > 0:\n            # Improvement! Accept new trajectories and lower regularization\n            redu_ratio_trace.append(redu_ratio)\n            cost_trace.append(total_cost)\n            x_trj = x_trj_new\n            u_trj = u_trj_new\n            regu *= 0.7\n        else:\n            # Reject new trajectories and increase regularization\n            regu *= 2.0\n            cost_trace.append(cost_trace[-1])\n            redu_ratio_trace.append(0)\n        regu = min(max(regu, min_regu), max_regu)\n        regu_trace.append(regu)\n        redu_trace.append(cost_redu)\n\n        # Early termination if expected improvement is small\n        if expected_cost_redu <= 1e-6:\n            break\n\n    return x_trj, u_trj, cost_trace, regu_trace, redu_ratio_trace, redu_trace\n\n\n# Setup problem and call iLQR\nx0 = np.array([-3.0, 1.0, -0.2, 0.0, 0.0])\nN = 50\nmax_iter = 50\nregu_init = 100\nx_trj, u_trj, cost_trace, regu_trace, redu_ratio_trace, redu_trace = run_ilqr(\n    x0, N, max_iter, regu_init\n)\n\n\nplt.figure(figsize=(9.5, 8))\n# Plot circle\ntheta = np.linspace(0, 2 * np.pi, 100)\nplt.plot(r * np.cos(theta), r * np.sin(theta), linewidth=5)\nax = plt.gca()\n\n# Plot resulting trajecotry of car\nplt.plot(x_trj[:, 0], x_trj[:, 1], linewidth=5)\nw = 2.0\nh = 1.0\n\n# Plot rectangles\nfor n in range(x_trj.shape[0]):\n    rect = mpl.patches.Rectangle((-w / 2, -h / 2), w, h, fill=False)\n    t = (\n        mpl.transforms.Affine2D()\n        .rotate_deg_around(0, 0, np.rad2deg(x_trj[n, 2]))\n        .translate(x_trj[n, 0], x_trj[n, 1])\n        + ax.transData\n    )\n    rect.set_transform(t)\n    ax.add_patch(rect)\nax.set_aspect(1)\nplt.ylim((-3, 3))\nplt.xlim((-4.5, 3))\nplt.tight_layout()"},{"block_group":"c46aec12b0ff4cb78b377cb52ace33b0","cell_type":"code","execution_count":null,"metadata":{"cell_id":"bef22c8cc596424c8c8565138f9cb8f7","deepnote_block_group":"c46aec12b0ff4cb78b377cb52ace33b0","deepnote_cell_type":"code","deepnote_sorting_key":"28","deepnote_source":"plt.subplots(figsize=(10, 6))\n# Plot results\nplt.subplot(2, 2, 1)\nplt.plot(cost_trace)\nplt.xlabel(\"# Iteration\")\nplt.ylabel(\"Total cost\")\nplt.title(\"Cost trace\")\n\nplt.subplot(2, 2, 2)\ndelta_opt = np.array(cost_trace) - cost_trace[-1]\nplt.plot(delta_opt)\nplt.yscale(\"log\")\nplt.xlabel(\"# Iteration\")\nplt.ylabel(\"Optimality gap\")\nplt.title(\"Convergence plot\")\n\nplt.subplot(2, 2, 3)\nplt.plot(redu_ratio_trace)\nplt.title(\"Ratio of actual reduction and expected reduction\")\nplt.ylabel(\"Reduction ratio\")\nplt.xlabel(\"# Iteration\")\n\nplt.subplot(2, 2, 4)\nplt.plot(regu_trace)\nplt.title(\"Regularization trace\")\nplt.ylabel(\"Regularization\")\nplt.xlabel(\"# Iteration\")\nplt.tight_layout()"},"outputs":[],"source":"plt.subplots(figsize=(10, 6))\n# Plot results\nplt.subplot(2, 2, 1)\nplt.plot(cost_trace)\nplt.xlabel(\"# Iteration\")\nplt.ylabel(\"Total cost\")\nplt.title(\"Cost trace\")\n\nplt.subplot(2, 2, 2)\ndelta_opt = np.array(cost_trace) - cost_trace[-1]\nplt.plot(delta_opt)\nplt.yscale(\"log\")\nplt.xlabel(\"# Iteration\")\nplt.ylabel(\"Optimality gap\")\nplt.title(\"Convergence plot\")\n\nplt.subplot(2, 2, 3)\nplt.plot(redu_ratio_trace)\nplt.title(\"Ratio of actual reduction and expected reduction\")\nplt.ylabel(\"Reduction ratio\")\nplt.xlabel(\"# Iteration\")\n\nplt.subplot(2, 2, 4)\nplt.plot(regu_trace)\nplt.title(\"Regularization trace\")\nplt.ylabel(\"Regularization\")\nplt.xlabel(\"# Iteration\")\nplt.tight_layout()"},{"block_group":"d32cbbef3745443c99e30ecaae8e8e56","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"ea337ac5d13a44c685c9f78730e0b68f","deepnote_block_group":"d32cbbef3745443c99e30ecaae8e8e56","deepnote_cell_type":"markdown","deepnote_sorting_key":"29","deepnote_source":"### Convergence Analysis\nYou can find some plots of the convergence traces captured throughout the iLQR solve process above. The convergence plot indicates that we have achieved superlinear convergence. In fact, iLQR achieves nearly second order convergence. In the case of linear convergence (e.g. gradient descent), the [graph would show a line](https://en.wikipedia.org/wiki/Rate_of_convergence). While the integrated regularization improves robustness it damps convergence in the early iteration steps. \n\nIn the ideal case, the expected reduction and the actual reduction should be the same, i.e. the reduction ratio remains around 1. If that is the case, the quadratic approximation of costs and linear approximation of the dynamics are very accurate. If the ratio becomes significantly lower than 1, the regularization needs to be increased and thus the stepsize reduced."},"source":"### Convergence Analysis\nYou can find some plots of the convergence traces captured throughout the iLQR solve process above. The convergence plot indicates that we have achieved superlinear convergence. In fact, iLQR achieves nearly second order convergence. In the case of linear convergence (e.g. gradient descent), the [graph would show a line](https://en.wikipedia.org/wiki/Rate_of_convergence). While the integrated regularization improves robustness it damps convergence in the early iteration steps. \n\nIn the ideal case, the expected reduction and the actual reduction should be the same, i.e. the reduction ratio remains around 1. If that is the case, the quadratic approximation of costs and linear approximation of the dynamics are very accurate. If the ratio becomes significantly lower than 1, the regularization needs to be increased and thus the stepsize reduced."},{"block_group":"25b135fee5d1469cace07c872c494f54","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"f44fceb0b7f84297abd794c5f39706e3","deepnote_block_group":"25b135fee5d1469cace07c872c494f54","deepnote_cell_type":"markdown","deepnote_sorting_key":"30","deepnote_source":"## Autograding\nYou can check your work by running the following cell."},"source":"## Autograding\nYou can check your work by running the following cell."},{"block_group":"680bbe630a044247a5b49748c492f31b","cell_type":"code","execution_count":null,"metadata":{"cell_id":"2f9b0ddc88ef418c95fc2a9c57ed1a68","deepnote_block_group":"680bbe630a044247a5b49748c492f31b","deepnote_cell_type":"code","deepnote_sorting_key":"31","deepnote_source":"from underactuated.exercises.grader import Grader\nfrom underactuated.exercises.trajopt.test_ilqr_driving import TestIlqrDriving\n\nGrader.grade_output([TestIlqrDriving], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"},"outputs":[],"source":"from underactuated.exercises.grader import Grader\nfrom underactuated.exercises.trajopt.test_ilqr_driving import TestIlqrDriving\n\nGrader.grade_output([TestIlqrDriving], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"}],
        "metadata": {"deepnote_notebook_id":"78495f94ef8442ca84e8772cf745722f"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }