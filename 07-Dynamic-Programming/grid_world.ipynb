{"cells":[{"block_group":"aa91593bd9cf484aab5bd1cb2c4f4c4f","cell_type":"markdown","execution_count":null,"metadata":{"id":"TKvYiJgnYExi","cell_id":"f9f1e1874836452f8bd6ad1bb1ad053e","deepnote_block_group":"aa91593bd9cf484aab5bd1cb2c4f4c4f","deepnote_cell_type":"markdown","deepnote_sorting_key":"0","deepnote_source":"This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n"},"source":"This notebook provides examples to go along with the [textbook](https://underactuated.csail.mit.edu/dp.html).  I recommend having both windows open, side-by-side!\n"},{"block_group":"55a0a79004f440f2874a00d902db3b36","cell_type":"code","execution_count":null,"metadata":{"id":"A4QOaw_zYLfI","cell_id":"4089c1f4add74523b09f0b0ce7681e53","deepnote_block_group":"55a0a79004f440f2874a00d902db3b36","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_source":"import matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import HTML, display\nfrom matplotlib import cm\nfrom pydrake.all import (\n    DynamicProgrammingOptions,\n    FittedValueIteration,\n    LinearSystem,\n    Simulator,\n)\n\nfrom underactuated import running_as_notebook"},"outputs":[],"source":"import matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import HTML, display\nfrom matplotlib import cm\nfrom pydrake.all import (\n    DynamicProgrammingOptions,\n    FittedValueIteration,\n    LinearSystem,\n    Simulator,\n)\n\nfrom underactuated import running_as_notebook"},{"block_group":"80b13159fd5f458083f18017a0ac490e","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"d01c87219d4545e48fd81fc058b3067d","deepnote_block_group":"80b13159fd5f458083f18017a0ac490e","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"# The Grid World\n\nThe setup here is *almost* identical as the simplest version described in the notes.  The only difference is that this agent is allowed to move diagonally in a single step; this is slightly easier to code since I can have two actions (one for left/right, and another for up/down), and write the dynamics as the trivial linear system ${\\bf x}[n+1] = {\\bf u}[n].$  Only the value iteration code needs to know that the states and actions are actually restricted to the integers. I also add a very large cost when the action would be diagonal, so that it is never chosen.\n\nThe obstacle (pit of despair) is provided by the method below.  Play around with it!  The rest of the code is mostly to support visualization."},"source":"# The Grid World\n\nThe setup here is *almost* identical as the simplest version described in the notes.  The only difference is that this agent is allowed to move diagonally in a single step; this is slightly easier to code since I can have two actions (one for left/right, and another for up/down), and write the dynamics as the trivial linear system ${\\bf x}[n+1] = {\\bf u}[n].$  Only the value iteration code needs to know that the states and actions are actually restricted to the integers. I also add a very large cost when the action would be diagonal, so that it is never chosen.\n\nThe obstacle (pit of despair) is provided by the method below.  Play around with it!  The rest of the code is mostly to support visualization."},{"block_group":"99aca835cadc4aafabfe5ca0199f8da5","cell_type":"code","execution_count":null,"metadata":{"cell_id":"2276175d6d144ac68b995bd72b3341e4","deepnote_block_group":"99aca835cadc4aafabfe5ca0199f8da5","deepnote_cell_type":"code","deepnote_sorting_key":"3","deepnote_source":"def grid_world_example():\n    time_step = 1\n    # TODO(russt): Support discrete-time systems in the dynamic programming\n    # code, and use this properly. For now, just cheat because I know how to\n    # make the discrete system as a continuous that will be discretized.\n    plant = LinearSystem(\n        A=np.zeros((2, 2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2, 2))\n    )\n    simulator = Simulator(plant)\n    options = DynamicProgrammingOptions()\n\n    xbins = range(0, 21)\n    ybins = range(0, 16)\n    state_grid = [set(xbins), set(ybins)]\n\n    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n\n    goal = [2, 8]\n\n    def obstacle(x):\n        return x[0] >= 6 and x[0] <= 8 and x[1] >= 4 and x[1] <= 7\n\n    [X, Y] = np.meshgrid(xbins, ybins)\n\n    frames = []\n\n    def draw(iteration, mesh, cost_to_go, policy):\n        J = np.reshape(cost_to_go, X.shape)\n        artists = [ax.imshow(J, cmap=cm.jet)]\n        artists += [\n            ax.quiver(\n                X,\n                Y,\n                np.reshape(policy[0], X.shape),\n                np.reshape(policy[1], Y.shape),\n                scale=1.4,\n                scale_units=\"x\",\n            )\n        ]\n        frames.append(artists)\n\n    if running_as_notebook:\n        options.visualization_callback = draw\n\n    def min_time_cost(context):\n        x = context.get_continuous_state_vector().CopyToVector()\n        x = np.round(x)\n        state_cost = 1\n        if obstacle(x):\n            state_cost = 10\n        if np.array_equal(x, goal):\n            state_cost = 0\n        u = plant.get_input_port(0).Eval(context)\n        action_cost = np.linalg.norm(u, 1)\n        if action_cost > 1:\n            action_cost = 10\n        return state_cost + action_cost\n\n    cost_function = min_time_cost\n    options.convergence_tol = 0.1\n\n    (fig, ax) = plt.subplots(figsize=(10, 6))\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_title(\"Cost-to-Go\")\n\n    policy, cost_to_go = FittedValueIteration(\n        simulator, cost_function, state_grid, input_grid, time_step, options\n    )\n\n    draw(\"Final\", None, cost_to_go, policy.get_output_values())\n\n    ax.invert_yaxis()\n    plt.colorbar(frames[-1][0])\n\n    print(\"generating animation...\")\n    # create animation using the animate() function\n    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n    plt.close(\"all\")\n\n    display(HTML(ani.to_jshtml()))\n\n\ngrid_world_example()"},"outputs":[],"source":"def grid_world_example():\n    time_step = 1\n    # TODO(russt): Support discrete-time systems in the dynamic programming\n    # code, and use this properly. For now, just cheat because I know how to\n    # make the discrete system as a continuous that will be discretized.\n    plant = LinearSystem(\n        A=np.zeros((2, 2)), B=np.eye(2), C=np.eye(2), D=np.zeros((2, 2))\n    )\n    simulator = Simulator(plant)\n    options = DynamicProgrammingOptions()\n\n    xbins = range(0, 21)\n    ybins = range(0, 16)\n    state_grid = [set(xbins), set(ybins)]\n\n    input_grid = [set([-1, 0, 1]), set([-1, 0, 1])]\n\n    goal = [2, 8]\n\n    def obstacle(x):\n        return x[0] >= 6 and x[0] <= 8 and x[1] >= 4 and x[1] <= 7\n\n    [X, Y] = np.meshgrid(xbins, ybins)\n\n    frames = []\n\n    def draw(iteration, mesh, cost_to_go, policy):\n        J = np.reshape(cost_to_go, X.shape)\n        artists = [ax.imshow(J, cmap=cm.jet)]\n        artists += [\n            ax.quiver(\n                X,\n                Y,\n                np.reshape(policy[0], X.shape),\n                np.reshape(policy[1], Y.shape),\n                scale=1.4,\n                scale_units=\"x\",\n            )\n        ]\n        frames.append(artists)\n\n    if running_as_notebook:\n        options.visualization_callback = draw\n\n    def min_time_cost(context):\n        x = context.get_continuous_state_vector().CopyToVector()\n        x = np.round(x)\n        state_cost = 1\n        if obstacle(x):\n            state_cost = 10\n        if np.array_equal(x, goal):\n            state_cost = 0\n        u = plant.get_input_port(0).Eval(context)\n        action_cost = np.linalg.norm(u, 1)\n        if action_cost > 1:\n            action_cost = 10\n        return state_cost + action_cost\n\n    cost_function = min_time_cost\n    options.convergence_tol = 0.1\n\n    (fig, ax) = plt.subplots(figsize=(10, 6))\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_title(\"Cost-to-Go\")\n\n    policy, cost_to_go = FittedValueIteration(\n        simulator, cost_function, state_grid, input_grid, time_step, options\n    )\n\n    draw(\"Final\", None, cost_to_go, policy.get_output_values())\n\n    ax.invert_yaxis()\n    plt.colorbar(frames[-1][0])\n\n    print(\"generating animation...\")\n    # create animation using the animate() function\n    ani = animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n    plt.close(\"all\")\n\n    display(HTML(ani.to_jshtml()))\n\n\ngrid_world_example()"},{"block_group":"7e69d5c4365b4aef9e9e2b6174f2f2db","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"25e74f9e10ca4de69b2daad4c984ecff","deepnote_block_group":"7e69d5c4365b4aef9e9e2b6174f2f2db","deepnote_cell_type":"markdown","deepnote_sorting_key":"4","deepnote_source":"Your turn.  Change the cost.  Change the obstacles."},"source":"Your turn.  Change the cost.  Change the obstacles."}],
        "metadata": {"deepnote_notebook_id":"e9243b6d9372466dbc4906a34c61ddc9"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }