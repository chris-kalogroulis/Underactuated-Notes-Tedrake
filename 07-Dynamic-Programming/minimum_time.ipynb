{"cells":[{"block_group":"c5a8fdbecead4707902db909afb29c82","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"fb223a5cf7d647b889d455ac48a39526","deepnote_block_group":"c5a8fdbecead4707902db909afb29c82","deepnote_cell_type":"markdown","deepnote_sorting_key":"0","deepnote_source":"# Value Iteration for Minimum Time Control"},"source":"# Value Iteration for Minimum Time Control"},{"block_group":"9563b0a6c7c4436dada7024ef741e8a0","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517256099,"execution_millis":930,"source_hash":"b4b4632f","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"b2000231bbbd48c78ce1e815e09345c5","deepnote_block_group":"9563b0a6c7c4436dada7024ef741e8a0","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_content_hash":"b4b4632f","deepnote_execution_started_at":"2023-02-16T03:14:16.099Z","deepnote_execution_finished_at":"2023-02-16T03:14:17.029Z","deepnote_source":"import numpy as np\nfrom IPython.display import HTML\nfrom pydrake.all import (\n    DiagramBuilder,\n    DynamicProgrammingOptions,\n    FittedValueIteration,\n    LeafSystem,\n    LinearSystem,\n    LogVectorOutput,\n    Simulator,\n)\n\nfrom underactuated.exercises.dp.minimum_time_utils import (\n    create_animation,\n    simulate_and_plot,\n)"},"outputs":[],"source":"import numpy as np\nfrom IPython.display import HTML\nfrom pydrake.all import (\n    DiagramBuilder,\n    DynamicProgrammingOptions,\n    FittedValueIteration,\n    LeafSystem,\n    LinearSystem,\n    LogVectorOutput,\n    Simulator,\n)\n\nfrom underactuated.exercises.dp.minimum_time_utils import (\n    create_animation,\n    simulate_and_plot,\n)"},{"block_group":"8f0111a2500b43a184427bb846d11921","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"e99d6825bc3a4ecf92c5b039d54aca5c","deepnote_block_group":"8f0111a2500b43a184427bb846d11921","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"## Problem Description\nIn this problem you will analyze the performance of the value-iteration algorithm on the minimum-time problem for the double integrator.\nDon't worry, the value iteration algorithm is provided by Drake, and you won't have to code it!\nYou will be asked to analyze the policy it produces and understand the algorithmic reasons behind the poor performance of the closed loop system.\nThen you will have to implement on your own the closed-form controller we have studied in class, and compare it with the one obtained numerically.\n\n**These are the main steps of the notebook (Items needed to be completed by you are marked as \"TODO\"):**\n1. Construct the double integrator system.\n2. Define the objective function for the minimum time problem (TODO).\n3. Run the value-iteration algorithm.\n4. Animate the intermediate steps of the algorithm.\n5. Simulate the double integrator in closed loop with the controller from the value iteration.\n6. Write down a controller that implements the closed form solution, and test it (TODO)."},"source":"## Problem Description\nIn this problem you will analyze the performance of the value-iteration algorithm on the minimum-time problem for the double integrator.\nDon't worry, the value iteration algorithm is provided by Drake, and you won't have to code it!\nYou will be asked to analyze the policy it produces and understand the algorithmic reasons behind the poor performance of the closed loop system.\nThen you will have to implement on your own the closed-form controller we have studied in class, and compare it with the one obtained numerically.\n\n**These are the main steps of the notebook (Items needed to be completed by you are marked as \"TODO\"):**\n1. Construct the double integrator system.\n2. Define the objective function for the minimum time problem (TODO).\n3. Run the value-iteration algorithm.\n4. Animate the intermediate steps of the algorithm.\n5. Simulate the double integrator in closed loop with the controller from the value iteration.\n6. Write down a controller that implements the closed form solution, and test it (TODO)."},{"block_group":"a3b739b2834a4609ae4486d3b82e2378","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"159ad6eea6864ae19a675505c1e930b3","deepnote_block_group":"a3b739b2834a4609ae4486d3b82e2378","deepnote_cell_type":"markdown","deepnote_sorting_key":"3","deepnote_source":"## Dynamics of the Double Integrator\nWe start by writing a function that returns the double-integrator system.\nWe write the dynamics is state-space linear form\n$$\\dot{\\mathbf{x}} = A \\mathbf{x} + B u,$$\nwhere $\\mathbf{x} = [q, \\dot{q}]^T$."},"source":"## Dynamics of the Double Integrator\nWe start by writing a function that returns the double-integrator system.\nWe write the dynamics is state-space linear form\n$$\\dot{\\mathbf{x}} = A \\mathbf{x} + B u,$$\nwhere $\\mathbf{x} = [q, \\dot{q}]^T$."},{"block_group":"098433a1fe0344feb71af540a8f89f92","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517257029,"execution_millis":0,"source_hash":"876aa04","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"7d5ea284489e4f778fe36a45ef9b4daa","deepnote_block_group":"098433a1fe0344feb71af540a8f89f92","deepnote_cell_type":"code","deepnote_sorting_key":"4","deepnote_content_hash":"876aa04","deepnote_execution_started_at":"2023-02-16T03:14:17.029Z","deepnote_execution_finished_at":"2023-02-16T03:14:17.029Z","deepnote_source":"# we write a function since we will need to call\n# this a handful of times\n\n\ndef get_double_integrator():\n    A = np.array([[0, 1], [0, 0]])\n    B = np.array([[0], [1]])\n    C = np.eye(2)\n    D = np.zeros((2, 1))\n    return LinearSystem(A, B, C, D)"},"outputs":[],"source":"# we write a function since we will need to call\n# this a handful of times\n\n\ndef get_double_integrator():\n    A = np.array([[0, 1], [0, 0]])\n    B = np.array([[0], [1]])\n    C = np.eye(2)\n    D = np.zeros((2, 1))\n    return LinearSystem(A, B, C, D)"},{"block_group":"255d1648e1114e5f8454cfb35f6cdd4d","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"b94bb3ed8c8e4702a5fd1fbdedd5438a","deepnote_block_group":"255d1648e1114e5f8454cfb35f6cdd4d","deepnote_cell_type":"markdown","deepnote_sorting_key":"5","deepnote_source":"## Implementation of integrand of the Cost Function\nRemember that the minimum-time objective can be written in integral form\n$$\\int_{0}^{\\infty} \\ell(\\mathbf{x}) dt,$$\nby defining\n$$\\ell(\\mathbf{x}) = \\begin{cases} 0 & \\text{if} \\quad \\mathbf{x} =0,\\\\ 1 & \\text{otherwise}. \\end{cases}$$\n(See also [the example from the textbook](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator).)\nImplement the integrand of cost function $$l(x)$$ using context as an argument.\n\n**Note**: To handle small numerical errors, the implementation of checking whether $$x=0$$ should be approximated using ```numpy``` function ```isclose``` instead of ```if x == 0```."},"source":"## Implementation of integrand of the Cost Function\nRemember that the minimum-time objective can be written in integral form\n$$\\int_{0}^{\\infty} \\ell(\\mathbf{x}) dt,$$\nby defining\n$$\\ell(\\mathbf{x}) = \\begin{cases} 0 & \\text{if} \\quad \\mathbf{x} =0,\\\\ 1 & \\text{otherwise}. \\end{cases}$$\n(See also [the example from the textbook](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator).)\nImplement the integrand of cost function $$l(x)$$ using context as an argument.\n\n**Note**: To handle small numerical errors, the implementation of checking whether $$x=0$$ should be approximated using ```numpy``` function ```isclose``` instead of ```if x == 0```."},{"block_group":"dd14d8b027d945668521effd5178a927","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517257030,"execution_millis":1,"source_hash":"f9cfac06","tags":[],"pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"9c560814d0544acfa72d9c94c5961402","deepnote_block_group":"dd14d8b027d945668521effd5178a927","deepnote_cell_type":"code","deepnote_sorting_key":"6","deepnote_content_hash":"f9cfac06","deepnote_execution_started_at":"2023-02-16T03:14:17.030Z","deepnote_execution_finished_at":"2023-02-16T03:14:17.031Z","deepnote_source":"def cost_function(context):\n    # Modify here to get the correct state vector value from context.\n    # Hint: Once you get a BasicVector in Drake, then call CopyToVector() to get a\n    # numpy array.\n    x = np.array([0.0, 0.0])\n    return 0  # Modify here to compute the cost function"},"outputs":[],"source":"def cost_function(context):\n    # Modify here to get the correct state vector value from context.\n    # Hint: Once you get a BasicVector in Drake, then call CopyToVector() to get a\n    # numpy array.\n    x = np.array([0.0, 0.0])\n    return 0  # Modify here to compute the cost function"},{"block_group":"68402bdaf3824e02bde8755bb90e2018","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"3b15b614df8f48f38435b5c215d39f4e","deepnote_block_group":"68402bdaf3824e02bde8755bb90e2018","deepnote_cell_type":"markdown","deepnote_sorting_key":"7","deepnote_source":"## Value Iteration Algorithm\nThe value iteration is implemented in the Drake function\n`FittedValueIteration`. Take some time to have a look at [its\ndocumentation](https://drake.mit.edu/doxygen_cxx/group__control.html#ga32d5768cb664f6d07fc58b4af536c45a),\nand to go through the description of this algorithm in [the\ntextbook](https://underactuated.csail.mit.edu/dp.html#barycentric). Before\nusing it, we need to construct an appropriate discretization of the state and\ninput space.\n\n**Important:** This code will work if you change the limits of the input to be\ndifferent from $u_{\\text{min}} = -1$ and $u_{\\text{max}} = 1$. However, be\naware that the closed-form solution we derived in class (and that you'll have\nto implement at the end of this notebook) is assuming that! It's not hard to\ngeneralize the closed-form solution to the case with generic bounds\n$u_{\\text{min}}$ and $u_{\\text{max}}$. But if you don't want to do that, do not\nchange `mesh['u_lim']` below!"},"source":"## Value Iteration Algorithm\nThe value iteration is implemented in the Drake function\n`FittedValueIteration`. Take some time to have a look at [its\ndocumentation](https://drake.mit.edu/doxygen_cxx/group__control.html#ga32d5768cb664f6d07fc58b4af536c45a),\nand to go through the description of this algorithm in [the\ntextbook](https://underactuated.csail.mit.edu/dp.html#barycentric). Before\nusing it, we need to construct an appropriate discretization of the state and\ninput space.\n\n**Important:** This code will work if you change the limits of the input to be\ndifferent from $u_{\\text{min}} = -1$ and $u_{\\text{max}} = 1$. However, be\naware that the closed-form solution we derived in class (and that you'll have\nto implement at the end of this notebook) is assuming that! It's not hard to\ngeneralize the closed-form solution to the case with generic bounds\n$u_{\\text{min}}$ and $u_{\\text{max}}$. But if you don't want to do that, do not\nchange `mesh['u_lim']` below!"},{"block_group":"68f843b09eeb477e92dfb696285886c6","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517257033,"execution_millis":0,"source_hash":"40da75c","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"95877e948b7745ffaf9f2276c4678275","deepnote_block_group":"68f843b09eeb477e92dfb696285886c6","deepnote_cell_type":"code","deepnote_sorting_key":"8","deepnote_content_hash":"40da75c","deepnote_execution_started_at":"2023-02-16T03:14:17.033Z","deepnote_execution_finished_at":"2023-02-16T03:14:17.033Z","deepnote_source":"# discretization mesh of state space, input space,\n# and time for the value-iteration algorithm\nmesh = {}\n\n# number of knot points in the grids\n# odd to have a point in the origin\nmesh[\"n_q\"] = 31  # do not exceed ~51/101\nmesh[\"n_qdot\"] = 31  # do not exceed ~51/101\nmesh[\"n_u\"] = 11  # don't exceed ~11/21\n\n# grid limits\nmesh[\"q_lim\"] = [-2.0, 2.0]\nmesh[\"qdot_lim\"] = [-2.0, 2.0]\nmesh[\"u_lim\"] = [-1.0, 1.0]  # do not change\n\n# axis discretization\nfor s in [\"q\", \"qdot\", \"u\"]:\n    mesh[f\"{s}_grid\"] = np.linspace(*mesh[f\"{s}_lim\"], mesh[f\"n_{s}\"])\n\n    # important: ensure that a knot point is in the origin\n    # otherwise there is no way the value iteration can converge\n    assert 0.0 in mesh[f\"{s}_grid\"]\n\n# time discretization in the value-iteration algorithm\nmesh[\"timestep\"] = 0.005"},"outputs":[],"source":"# discretization mesh of state space, input space,\n# and time for the value-iteration algorithm\nmesh = {}\n\n# number of knot points in the grids\n# odd to have a point in the origin\nmesh[\"n_q\"] = 31  # do not exceed ~51/101\nmesh[\"n_qdot\"] = 31  # do not exceed ~51/101\nmesh[\"n_u\"] = 11  # don't exceed ~11/21\n\n# grid limits\nmesh[\"q_lim\"] = [-2.0, 2.0]\nmesh[\"qdot_lim\"] = [-2.0, 2.0]\nmesh[\"u_lim\"] = [-1.0, 1.0]  # do not change\n\n# axis discretization\nfor s in [\"q\", \"qdot\", \"u\"]:\n    mesh[f\"{s}_grid\"] = np.linspace(*mesh[f\"{s}_lim\"], mesh[f\"n_{s}\"])\n\n    # important: ensure that a knot point is in the origin\n    # otherwise there is no way the value iteration can converge\n    assert 0.0 in mesh[f\"{s}_grid\"]\n\n# time discretization in the value-iteration algorithm\nmesh[\"timestep\"] = 0.005"},{"block_group":"dd11504121c4431bb31ba2c51c7e322c","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"3d18ce1115ee473d9d600050475f7bef","deepnote_block_group":"dd11504121c4431bb31ba2c51c7e322c","deepnote_cell_type":"markdown","deepnote_sorting_key":"9","deepnote_source":"In the following cell we wrap Drake's `FittedValueIteration` function with a function we call `run_value_iteration`.\nThis returns the optimal value function, the optimal controller, and all the data we need for the upcoming animation."},"source":"In the following cell we wrap Drake's `FittedValueIteration` function with a function we call `run_value_iteration`.\nThis returns the optimal value function, the optimal controller, and all the data we need for the upcoming animation."},{"block_group":"81207852406843e3b01b47cffbaf8541","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517257034,"execution_millis":4,"source_hash":"d0f5e75f","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"6270037bec1c43038e372afb91d064ee","deepnote_block_group":"81207852406843e3b01b47cffbaf8541","deepnote_cell_type":"code","deepnote_sorting_key":"10","deepnote_content_hash":"d0f5e75f","deepnote_execution_started_at":"2023-02-16T03:14:17.034Z","deepnote_execution_finished_at":"2023-02-16T03:14:17.038Z","deepnote_source":"def run_value_iteration(cost_function, mesh, max_iter=10000):\n    # to create an animation, we store the values of\n    # the cost to go and the optimal policy for each\n    # iteration of the value-iteration algorithm\n    J_grid = []\n    pi_grid = []\n\n    # callback from the value-iteration algorithm\n    # that saves the intermediate values of J and pi\n    # and that ensures we do not exceed max_iter\n    # (iteration number i starts from 1)\n    def callback(i, unused, J, pi):\n        # check max iter is not exceeded\n        if i > max_iter:\n            raise RuntimeError(\n                f\"Value-iteration algorithm did not converge within {max_iter} iterations.\"\n            )\n\n        # store cost to go for iteration i\n        # the 'F' order facilitates the plot phase\n        J_grid.append(np.reshape(J, (mesh[\"n_q\"], mesh[\"n_qdot\"]), order=\"F\"))\n        pi_grid.append(np.reshape(pi, (mesh[\"n_q\"], mesh[\"n_qdot\"]), order=\"F\"))\n\n    # set up a simulation\n    simulator = Simulator(get_double_integrator())\n\n    # grids for the value-iteration algorithm\n    state_grid = [set(mesh[\"q_grid\"]), set(mesh[\"qdot_grid\"])]\n    input_grid = [set(mesh[\"u_grid\"])]\n\n    # add custom callback function as a visualization_callback\n    options = DynamicProgrammingOptions()\n    options.visualization_callback = callback\n\n    # run value-iteration algorithm\n    policy, cost_to_go = FittedValueIteration(\n        simulator,\n        cost_function,\n        state_grid,\n        input_grid,\n        mesh[\"timestep\"],\n        options,\n    )\n\n    # recast J and pi from lists to 3d arrays\n    J_grid = np.dstack(J_grid)\n    pi_grid = np.dstack(pi_grid)\n\n    return policy, cost_to_go, J_grid, pi_grid"},"outputs":[],"source":"def run_value_iteration(cost_function, mesh, max_iter=10000):\n    # to create an animation, we store the values of\n    # the cost to go and the optimal policy for each\n    # iteration of the value-iteration algorithm\n    J_grid = []\n    pi_grid = []\n\n    # callback from the value-iteration algorithm\n    # that saves the intermediate values of J and pi\n    # and that ensures we do not exceed max_iter\n    # (iteration number i starts from 1)\n    def callback(i, unused, J, pi):\n        # check max iter is not exceeded\n        if i > max_iter:\n            raise RuntimeError(\n                f\"Value-iteration algorithm did not converge within {max_iter} iterations.\"\n            )\n\n        # store cost to go for iteration i\n        # the 'F' order facilitates the plot phase\n        J_grid.append(np.reshape(J, (mesh[\"n_q\"], mesh[\"n_qdot\"]), order=\"F\"))\n        pi_grid.append(np.reshape(pi, (mesh[\"n_q\"], mesh[\"n_qdot\"]), order=\"F\"))\n\n    # set up a simulation\n    simulator = Simulator(get_double_integrator())\n\n    # grids for the value-iteration algorithm\n    state_grid = [set(mesh[\"q_grid\"]), set(mesh[\"qdot_grid\"])]\n    input_grid = [set(mesh[\"u_grid\"])]\n\n    # add custom callback function as a visualization_callback\n    options = DynamicProgrammingOptions()\n    options.visualization_callback = callback\n\n    # run value-iteration algorithm\n    policy, cost_to_go = FittedValueIteration(\n        simulator,\n        cost_function,\n        state_grid,\n        input_grid,\n        mesh[\"timestep\"],\n        options,\n    )\n\n    # recast J and pi from lists to 3d arrays\n    J_grid = np.dstack(J_grid)\n    pi_grid = np.dstack(pi_grid)\n\n    return policy, cost_to_go, J_grid, pi_grid"},{"block_group":"5883c186a07945dfbfdc33261fa77b29","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"bb8125828dcd4dcca7e38a1e9a298985","deepnote_block_group":"5883c186a07945dfbfdc33261fa77b29","deepnote_cell_type":"markdown","deepnote_sorting_key":"11","deepnote_source":"## Animation of the Value-Iteration Algorithm\nThe animation of the value-iteration is coded mainly using matplotlib. If you are interested, feel free to check support function `create_animation` provided in [`minimum_time_utils.py`](https://github.com/RussTedrake/underactuated/blob/master/underactuated/exercises/dp/minimum_time_utils.py).\nWhat it does can be summarized as follows:\n- runs value iteration,\n- initializes an empty 3D surface plot for the value function and the policy,\n- creates the function `update_surf` that when called updates the surface plots from the previous point,\n- creates a fancy animation by calling `update_surf` many times.\n\nThis animation is built for the purpose of visualizing value-iteration, therefore, we include supporting functions in a separate file and hope you can appreciate the relevant final results!"},"source":"## Animation of the Value-Iteration Algorithm\nThe animation of the value-iteration is coded mainly using matplotlib. If you are interested, feel free to check support function `create_animation` provided in [`minimum_time_utils.py`](https://github.com/RussTedrake/underactuated/blob/master/underactuated/exercises/dp/minimum_time_utils.py).\nWhat it does can be summarized as follows:\n- runs value iteration,\n- initializes an empty 3D surface plot for the value function and the policy,\n- creates the function `update_surf` that when called updates the surface plots from the previous point,\n- creates a fancy animation by calling `update_surf` many times.\n\nThis animation is built for the purpose of visualizing value-iteration, therefore, we include supporting functions in a separate file and hope you can appreciate the relevant final results!"},{"block_group":"551be7e283fa4c23a3e8a859aa1f774d","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517257038,"execution_millis":11928,"source_hash":"e609b59","tags":[],"pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"ad7858fcf26b4206aad22d4f9168514d","deepnote_block_group":"551be7e283fa4c23a3e8a859aa1f774d","deepnote_cell_type":"code","deepnote_sorting_key":"12","deepnote_content_hash":"e609b59","deepnote_execution_started_at":"2023-02-16T03:14:17.038Z","deepnote_execution_finished_at":"2023-02-16T03:14:28.966Z","deepnote_source":"policy, cost_to_go, J_grid, pi_grid = run_value_iteration(cost_function, mesh)\nanimation = create_animation(J_grid, pi_grid, mesh)\nHTML(animation.to_jshtml())"},"outputs":[],"source":"policy, cost_to_go, J_grid, pi_grid = run_value_iteration(cost_function, mesh)\nanimation = create_animation(J_grid, pi_grid, mesh)\nHTML(animation.to_jshtml())"},{"block_group":"34c76a31d69b4904b70bf3a94ebaebe8","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"c0419d551e81460e984b27c24e026972","deepnote_block_group":"34c76a31d69b4904b70bf3a94ebaebe8","deepnote_cell_type":"markdown","deepnote_sorting_key":"13","deepnote_source":"## Performance of the Value-Iteration Policy\nValue iteration is an extremely powerful and very general algorithm.\nHowever, its performances in solving \"bang-bang\" problems (i.e. problems where the control is always at the bounds) can be very poor.\nIn this section we simulate the double integrator in closed-loop with the approximated optimal policy.\nWe'll see that things do not go exactly how we expect..."},"source":"## Performance of the Value-Iteration Policy\nValue iteration is an extremely powerful and very general algorithm.\nHowever, its performances in solving \"bang-bang\" problems (i.e. problems where the control is always at the bounds) can be very poor.\nIn this section we simulate the double integrator in closed-loop with the approximated optimal policy.\nWe'll see that things do not go exactly how we expect..."},{"block_group":"548bd620429349809342578f43ce460a","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517269661,"execution_millis":19206916,"source_hash":"552e66a9","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"518fc1708a8b48a4b34067a2bb073ba0","deepnote_block_group":"548bd620429349809342578f43ce460a","deepnote_cell_type":"code","deepnote_sorting_key":"14","deepnote_content_hash":"552e66a9","deepnote_execution_started_at":"2023-02-16T03:14:29.661Z","deepnote_execution_finished_at":"2023-02-16T08:34:36.577Z","deepnote_source":"# function that simulates the double integrator\n# starting from the state (q0, qdot0) for sim_time\n# seconds in closed loop with the passed controller\n\n\ndef simulate(q0, qdot0, sim_time, controller):\n    # initialize block diagram\n    builder = DiagramBuilder()\n\n    # add system and controller\n    double_integrator = builder.AddSystem(get_double_integrator())\n    controller = builder.AddSystem(controller)\n\n    # wirw system and controller\n    builder.Connect(double_integrator.get_output_port(0), controller.get_input_port(0))\n    builder.Connect(controller.get_output_port(0), double_integrator.get_input_port(0))\n\n    # measure double-integrator state and input\n    state_logger = LogVectorOutput(double_integrator.get_output_port(0), builder)\n    input_logger = LogVectorOutput(controller.get_output_port(0), builder)\n\n    # finalize block diagram\n    diagram = builder.Build()\n\n    # instantiate simulator\n    simulator = Simulator(diagram)\n    simulator.set_publish_every_time_step(False)  # makes sim faster\n\n    # set initial conditions\n    context = simulator.get_mutable_context()\n    context.SetContinuousState([q0, qdot0])\n\n    # run simulation\n    simulator.AdvanceTo(sim_time)\n\n    # unpack sim results\n    q_sim, qdot_sim = state_logger.FindLog(context).data()\n    u_sim = input_logger.FindLog(context).data().flatten()\n    t_sim = state_logger.FindLog(context).sample_times()\n\n    return q_sim, qdot_sim, u_sim, t_sim"},"outputs":[],"source":"# function that simulates the double integrator\n# starting from the state (q0, qdot0) for sim_time\n# seconds in closed loop with the passed controller\n\n\ndef simulate(q0, qdot0, sim_time, controller):\n    # initialize block diagram\n    builder = DiagramBuilder()\n\n    # add system and controller\n    double_integrator = builder.AddSystem(get_double_integrator())\n    controller = builder.AddSystem(controller)\n\n    # wirw system and controller\n    builder.Connect(double_integrator.get_output_port(0), controller.get_input_port(0))\n    builder.Connect(controller.get_output_port(0), double_integrator.get_input_port(0))\n\n    # measure double-integrator state and input\n    state_logger = LogVectorOutput(double_integrator.get_output_port(0), builder)\n    input_logger = LogVectorOutput(controller.get_output_port(0), builder)\n\n    # finalize block diagram\n    diagram = builder.Build()\n\n    # instantiate simulator\n    simulator = Simulator(diagram)\n    simulator.set_publish_every_time_step(False)  # makes sim faster\n\n    # set initial conditions\n    context = simulator.get_mutable_context()\n    context.SetContinuousState([q0, qdot0])\n\n    # run simulation\n    simulator.AdvanceTo(sim_time)\n\n    # unpack sim results\n    q_sim, qdot_sim = state_logger.FindLog(context).data()\n    u_sim = input_logger.FindLog(context).data().flatten()\n    t_sim = state_logger.FindLog(context).sample_times()\n\n    return q_sim, qdot_sim, u_sim, t_sim"},{"block_group":"2b523a55a5e94b479b4e5cfd89a05327","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"fbb014d4eb74464c91519136f4dae3a2","deepnote_block_group":"2b523a55a5e94b479b4e5cfd89a05327","deepnote_cell_type":"markdown","deepnote_sorting_key":"15","deepnote_source":"In order to properly visualize the results of the simulator above we need a bunch of helper functions. Since they are not directly relevant to drake simulation or value iteration algorithm, we included them in [`minimum_time_utils.py`](underactuated/exercises/dp/minimum_time_utils.py). Feel free to check the detailed implementation if you are interested."},"source":"In order to properly visualize the results of the simulator above we need a bunch of helper functions. Since they are not directly relevant to drake simulation or value iteration algorithm, we included them in [`minimum_time_utils.py`](underactuated/exercises/dp/minimum_time_utils.py). Feel free to check the detailed implementation if you are interested."},{"block_group":"445187f4bb074185bd12ea29d9b3f8c9","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"799a9dcb55e6469da1269059abcdc362","deepnote_block_group":"445187f4bb074185bd12ea29d9b3f8c9","deepnote_cell_type":"markdown","deepnote_sorting_key":"16","deepnote_source":"We are finally ready to simulate and plot the trajectories of the double integrator controlled by the value-iteration policy.\nRunning the following cell you'll see two plots:\n- The plot of the state-space trajectory of the double integrator superimposed to the level plot of the policy.\nIn the red regions the controller selects the input $u=1$ (full gas), in the blue regions it selects $u=-1$ (full brake). The are in between approximates the quadratic boundaries we have seen in class, and are due to the discretization of the state space.\n- The plot of the control force as a function of time.\n\nIs this the optimal policy we expected to see?\nTake your time to understand why these plots look so strange!\nDoes this get any better if you increase the number of knot points (finer discretization of $q$ and $\\dot{q}$)?\nIf no, why?\n(Questions not graded, do not submit.)"},"source":"We are finally ready to simulate and plot the trajectories of the double integrator controlled by the value-iteration policy.\nRunning the following cell you'll see two plots:\n- The plot of the state-space trajectory of the double integrator superimposed to the level plot of the policy.\nIn the red regions the controller selects the input $u=1$ (full gas), in the blue regions it selects $u=-1$ (full brake). The are in between approximates the quadratic boundaries we have seen in class, and are due to the discretization of the state space.\n- The plot of the control force as a function of time.\n\nIs this the optimal policy we expected to see?\nTake your time to understand why these plots look so strange!\nDoes this get any better if you increase the number of knot points (finer discretization of $q$ and $\\dot{q}$)?\nIf no, why?\n(Questions not graded, do not submit.)"},{"block_group":"5771b7fa2e8a4bf9996799ec153b2033","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517269664,"execution_millis":1813,"source_hash":"c612ede8","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"b45d742851234c318d1fb61db291c7ab","deepnote_block_group":"5771b7fa2e8a4bf9996799ec153b2033","deepnote_cell_type":"code","deepnote_sorting_key":"17","deepnote_content_hash":"c612ede8","deepnote_execution_started_at":"2023-02-16T03:14:29.664Z","deepnote_execution_finished_at":"2023-02-16T03:14:31.477Z","deepnote_source":"# initial state\nq0 = -1.0\nqdot0 = 0.0\n\n# verify that the given initial state is inside the value-iteration grid\nassert mesh[\"q_lim\"][0] <= q0 <= mesh[\"q_lim\"][1]\nassert mesh[\"qdot_lim\"][0] <= qdot0 <= mesh[\"qdot_lim\"][1]\n\n# duration of the simulation in seconds\nsim_time = 5.0\n\n# sim and plot\npolicy = run_value_iteration(cost_function, mesh)[0]\nsimulate_and_plot(q0, qdot0, sim_time, policy, mesh[\"u_lim\"], simulate=simulate)"},"outputs":[],"source":"# initial state\nq0 = -1.0\nqdot0 = 0.0\n\n# verify that the given initial state is inside the value-iteration grid\nassert mesh[\"q_lim\"][0] <= q0 <= mesh[\"q_lim\"][1]\nassert mesh[\"qdot_lim\"][0] <= qdot0 <= mesh[\"qdot_lim\"][1]\n\n# duration of the simulation in seconds\nsim_time = 5.0\n\n# sim and plot\npolicy = run_value_iteration(cost_function, mesh)[0]\nsimulate_and_plot(q0, qdot0, sim_time, policy, mesh[\"u_lim\"], simulate=simulate)"},{"block_group":"676d1b96ba4542b0ac67eb88009f4e45","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"21870533ab6c44a99b5cc27807d50f78","deepnote_block_group":"676d1b96ba4542b0ac67eb88009f4e45","deepnote_cell_type":"markdown","deepnote_sorting_key":"18","deepnote_source":"## Implementation of the Closed-Form Solution\nSince value iteration didn't give us the results we wanted, in the next cell we ask you to implement [the closed-form solution we've derived in class](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator).\nNote that in class we assumed the input to be bounded between $-1$ and $1$, so you can either do the math and generalize that result to generic bounds $u_{\\text{min}} < 0$ and $u_{\\text{max}} > 0$ (not hard), or double check that `mesh['u_lim']` is still set to `[-1., 1.]`.\n\n**Note 1:**\nTo help you, we already partially filled the function.\nIn a small neighborhood of the origin we return $u = - \\dot{q} - q$, even if the theoretical solution would say $u = 0$.\nThis gives the closed-loop dynamics $m \\ddot{q} = - q - \\dot{q}$ which makes the origin a stable equilibrium.\nThis trick prevents the controller from chattering wildly between $u_{\\text{max}}$ and $u_{\\text{min}}$ because of small numerical errors.\nDo not cancel it.\n\n**Note 2:**\nTo complete this function with [the control law from the textbook](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator)\nyou need to write two conditions on the state $[q, \\dot{q}]^T$: one for the full-gas region and one for the full-brake region.\nNotice that, momentarily, the function always returns $u = u_{\\text{max}}$ if the state is not close to the origin."},"source":"## Implementation of the Closed-Form Solution\nSince value iteration didn't give us the results we wanted, in the next cell we ask you to implement [the closed-form solution we've derived in class](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator).\nNote that in class we assumed the input to be bounded between $-1$ and $1$, so you can either do the math and generalize that result to generic bounds $u_{\\text{min}} < 0$ and $u_{\\text{max}} > 0$ (not hard), or double check that `mesh['u_lim']` is still set to `[-1., 1.]`.\n\n**Note 1:**\nTo help you, we already partially filled the function.\nIn a small neighborhood of the origin we return $u = - \\dot{q} - q$, even if the theoretical solution would say $u = 0$.\nThis gives the closed-loop dynamics $m \\ddot{q} = - q - \\dot{q}$ which makes the origin a stable equilibrium.\nThis trick prevents the controller from chattering wildly between $u_{\\text{max}}$ and $u_{\\text{min}}$ because of small numerical errors.\nDo not cancel it.\n\n**Note 2:**\nTo complete this function with [the control law from the textbook](https://underactuated.csail.mit.edu/dp.html#minimum_time_double_integrator)\nyou need to write two conditions on the state $[q, \\dot{q}]^T$: one for the full-gas region and one for the full-brake region.\nNotice that, momentarily, the function always returns $u = u_{\\text{max}}$ if the state is not close to the origin."},{"block_group":"aa26290a547c48569f7228c4064a9672","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517271475,"execution_millis":4,"source_hash":"4c55d5bd","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"9bb3289005804de89be10a260e5b4224","deepnote_block_group":"aa26290a547c48569f7228c4064a9672","deepnote_cell_type":"code","deepnote_sorting_key":"19","deepnote_content_hash":"4c55d5bd","deepnote_execution_started_at":"2023-02-16T03:14:31.475Z","deepnote_execution_finished_at":"2023-02-16T03:14:31.479Z","deepnote_source":"def policy_closed_form(q, qdot, atol=1.0e-2):\n    # system in a neighborhood of the origin\n    # up to the absolute tolerance atol\n    x_norm = np.linalg.norm([q, qdot])\n    if np.isclose(x_norm, 0.0, atol=atol):\n        # little trick, do not modify: use a stabilizing controller in the\n        # neighborhood of the origin to prevent wild chattering\n        return -q - qdot\n\n    # full-brake region\n    # check if the state of the system is\n    # such that u must be set to -1\n    elif False:  # modify here\n        return mesh[\"u_lim\"][0]\n\n    # full-gas region\n    # if all the others do not apply,\n    # u must be set to 1\n    else:  # modify here\n        return mesh[\"u_lim\"][1]"},"outputs":[],"source":"def policy_closed_form(q, qdot, atol=1.0e-2):\n    # system in a neighborhood of the origin\n    # up to the absolute tolerance atol\n    x_norm = np.linalg.norm([q, qdot])\n    if np.isclose(x_norm, 0.0, atol=atol):\n        # little trick, do not modify: use a stabilizing controller in the\n        # neighborhood of the origin to prevent wild chattering\n        return -q - qdot\n\n    # full-brake region\n    # check if the state of the system is\n    # such that u must be set to -1\n    elif False:  # modify here\n        return mesh[\"u_lim\"][0]\n\n    # full-gas region\n    # if all the others do not apply,\n    # u must be set to 1\n    else:  # modify here\n        return mesh[\"u_lim\"][1]"},{"block_group":"fc6d9b09be9049eb9d8076621010017b","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"11cb7844994642e8867fe38f32b90c93","deepnote_block_group":"fc6d9b09be9049eb9d8076621010017b","deepnote_cell_type":"markdown","deepnote_sorting_key":"20","deepnote_source":"Now we just encapsulate the function you wrote in a Drake `LeafSystem` that can be sent to the simulator.\nDoes this state trajectory and this control signal look more reasonable than the ones from the value-iteration algorithm? (Question not graded, do not submit.)"},"source":"Now we just encapsulate the function you wrote in a Drake `LeafSystem` that can be sent to the simulator.\nDoes this state trajectory and this control signal look more reasonable than the ones from the value-iteration algorithm? (Question not graded, do not submit.)"},{"block_group":"a5480160ddff40398e83ed7dbfb7f4f0","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517271501,"execution_millis":3672,"source_hash":"ccdd00cd","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"25059bcbad8f4c8bb319214bf35f47b6","deepnote_block_group":"a5480160ddff40398e83ed7dbfb7f4f0","deepnote_cell_type":"code","deepnote_sorting_key":"21","deepnote_content_hash":"ccdd00cd","deepnote_execution_started_at":"2023-02-16T03:14:31.501Z","deepnote_execution_finished_at":"2023-02-16T03:14:35.173Z","deepnote_source":"# controller which implements the closed-form solution\n\n\nclass ClosedFormController(LeafSystem):\n    # two inputs (system state)\n    # one output (system input)\n    def __init__(self):\n        LeafSystem.__init__(self)\n        self.DeclareVectorInputPort(\"x\", 2)\n        self.DeclareVectorOutputPort(\"u\", 1, self.DoCalcVectorOutput)\n\n    # just evaluate the function above\n    def DoCalcVectorOutput(self, context, u):\n        x = self.get_input_port(0).Eval(context)\n        u.SetAtIndex(0, policy_closed_form(*x))\n\n\n# sim and plot\nsimulate_and_plot(\n    q0,\n    qdot0,\n    sim_time,\n    ClosedFormController(),\n    mesh[\"u_lim\"],\n    simulate=simulate,\n)"},"outputs":[],"source":"# controller which implements the closed-form solution\n\n\nclass ClosedFormController(LeafSystem):\n    # two inputs (system state)\n    # one output (system input)\n    def __init__(self):\n        LeafSystem.__init__(self)\n        self.DeclareVectorInputPort(\"x\", 2)\n        self.DeclareVectorOutputPort(\"u\", 1, self.DoCalcVectorOutput)\n\n    # just evaluate the function above\n    def DoCalcVectorOutput(self, context, u):\n        x = self.get_input_port(0).Eval(context)\n        u.SetAtIndex(0, policy_closed_form(*x))\n\n\n# sim and plot\nsimulate_and_plot(\n    q0,\n    qdot0,\n    sim_time,\n    ClosedFormController(),\n    mesh[\"u_lim\"],\n    simulate=simulate,\n)"},{"block_group":"4395e64402604555882cdfa755bcc6a7","cell_type":"markdown","execution_count":null,"metadata":{"pycharm":{"name":"#%% md\n"},"cell_id":"8bb7581311a14f3883fb2a965189ccb3","deepnote_block_group":"4395e64402604555882cdfa755bcc6a7","deepnote_cell_type":"markdown","deepnote_sorting_key":"22","deepnote_source":"## Autograding\nYou can check your work by running the following cell:"},"source":"## Autograding\nYou can check your work by running the following cell:"},{"block_group":"aa7669fab275461895cd9f8c08d4aede","cell_type":"code","execution_count":null,"metadata":{"execution_start":1676517275172,"execution_millis":587,"source_hash":"57d0c95b","pycharm":{"name":"#%%\n"},"deepnote_to_be_reexecuted":false,"cell_id":"d05d017f717f4457b3b31a986244ed74","deepnote_block_group":"aa7669fab275461895cd9f8c08d4aede","deepnote_cell_type":"code","deepnote_sorting_key":"23","deepnote_content_hash":"57d0c95b","deepnote_execution_started_at":"2023-02-16T03:14:35.172Z","deepnote_execution_finished_at":"2023-02-16T03:14:35.759Z","deepnote_source":"from underactuated.exercises.dp.test_minimum_time import TestMinimumTime\nfrom underactuated.exercises.grader import Grader\n\nGrader.grade_output([TestMinimumTime], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"},"outputs":[],"source":"from underactuated.exercises.dp.test_minimum_time import TestMinimumTime\nfrom underactuated.exercises.grader import Grader\n\nGrader.grade_output([TestMinimumTime], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"}],
        "metadata": {"deepnote_notebook_id":"29192182a1384f3987b5619953ff0a1d"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }