{"cells":[{"block_group":"2dd04a7e788445abad3de20cc5c7b7b9","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"88d6bc01125d4282970847e8ed238ad8","deepnote_block_group":"2dd04a7e788445abad3de20cc5c7b7b9","deepnote_cell_type":"markdown","deepnote_sorting_key":"0","deepnote_source":"# ROA Estimation for the Time-Reversed Van der Pol Oscillator"},"source":"# ROA Estimation for the Time-Reversed Van der Pol Oscillator"},{"block_group":"09ef0848bb8345d897d220a6a1354df6","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230395235,"execution_millis":3,"source_hash":"aa09f44","deepnote_to_be_reexecuted":false,"cell_id":"dce18eb0dd174d1d9f59bd9f584d199b","deepnote_block_group":"09ef0848bb8345d897d220a6a1354df6","deepnote_cell_type":"code","deepnote_sorting_key":"1","deepnote_content_hash":"aa09f44","deepnote_execution_started_at":"2023-03-07T23:06:35.235Z","deepnote_execution_finished_at":"2023-03-07T23:06:35.238Z","deepnote_source":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import rcParams\nfrom pydrake.all import (\n    MathematicalProgram,\n    RealContinuousLyapunovEquation,\n    Solve,\n    Variables,\n)\nfrom pydrake.examples import VanDerPolOscillator\n\nfrom underactuated import plot_2d_phase_portrait\n\n# increase default size matplotlib figures\nrcParams[\"figure.figsize\"] = (6, 6)"},"outputs":[],"source":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import rcParams\nfrom pydrake.all import (\n    MathematicalProgram,\n    RealContinuousLyapunovEquation,\n    Solve,\n    Variables,\n)\nfrom pydrake.examples import VanDerPolOscillator\n\nfrom underactuated import plot_2d_phase_portrait\n\n# increase default size matplotlib figures\nrcParams[\"figure.figsize\"] = (6, 6)"},{"block_group":"73f75ad88eb94e3eb922abebaf18371a","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"f125ada0cfcb47a489ddc64d4cea3085","deepnote_block_group":"73f75ad88eb94e3eb922abebaf18371a","deepnote_cell_type":"markdown","deepnote_sorting_key":"2","deepnote_source":"## Problem Description\nIn this notebook we will study the time-reversed Van der Pol oscillator.\nThe equations of motion for this system are polynomial, and read as follows:\n$$\\begin{aligned}\\dot x_1 &= - x_2, \\\\ \\dot x_2 &= x_1 + (x_1^2 - 1) x_2.\\end{aligned}$$\nWe compactly represent the latter as $\\dot{\\mathbf{x}} = f(\\mathbf{x})$, with $\\mathbf{x} = [x_1, x_2]^T$."},"source":"## Problem Description\nIn this notebook we will study the time-reversed Van der Pol oscillator.\nThe equations of motion for this system are polynomial, and read as follows:\n$$\\begin{aligned}\\dot x_1 &= - x_2, \\\\ \\dot x_2 &= x_1 + (x_1^2 - 1) x_2.\\end{aligned}$$\nWe compactly represent the latter as $\\dot{\\mathbf{x}} = f(\\mathbf{x})$, with $\\mathbf{x} = [x_1, x_2]^T$."},{"block_group":"311bf19310c84879a72e4a8a0384d078","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678228164691,"execution_millis":6,"source_hash":"b3959b58","deepnote_to_be_reexecuted":false,"cell_id":"05f6b92fcb3241219ab2c1d1eaa38ce0","deepnote_block_group":"311bf19310c84879a72e4a8a0384d078","deepnote_cell_type":"code","deepnote_sorting_key":"3","deepnote_content_hash":"b3959b58","deepnote_execution_started_at":"2023-03-07T22:29:24.691Z","deepnote_execution_finished_at":"2023-03-07T22:29:24.697Z","deepnote_source":"# function that implements the time-reversed Van der Pol dynamics\nf = lambda x: [-x[1], x[0] + (x[0] ** 2 - 1) * x[1]]"},"outputs":[],"source":"# function that implements the time-reversed Van der Pol dynamics\nf = lambda x: [-x[1], x[0] + (x[0] ** 2 - 1) * x[1]]"},{"block_group":"5ec50db5d14846fc86c0b80fd64ddd45","cell_type":"markdown","execution_count":null,"metadata":{"tags":[],"cell_id":"b8c64500e5fa4b509ed0bfd92eac102c","deepnote_block_group":"5ec50db5d14846fc86c0b80fd64ddd45","deepnote_cell_type":"markdown","deepnote_sorting_key":"4","deepnote_source":"Here is the phase portrait of the time-reversed Van der Pol oscillator."},"source":"Here is the phase portrait of the time-reversed Van der Pol oscillator."},{"block_group":"a678c0ececec4af89e455889890c555b","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678228167554,"execution_millis":949,"source_hash":"4a379c87","deepnote_to_be_reexecuted":false,"cell_id":"870df9a895c2463784b2f51bca3bb045","deepnote_block_group":"a678c0ececec4af89e455889890c555b","deepnote_cell_type":"code","deepnote_sorting_key":"5","deepnote_content_hash":"4a379c87","deepnote_execution_started_at":"2023-03-07T22:29:27.554Z","deepnote_execution_finished_at":"2023-03-07T22:29:28.503Z","deepnote_source":"# compute and plot the unstable periodic orbit\nlimit_cycle = VanDerPolOscillator.CalcLimitCycle()\nplt.plot(\n    limit_cycle[0],\n    limit_cycle[1],\n    color=\"b\",\n    linewidth=3,\n    label=\"ROA boundary\",\n)\nplt.legend(loc=1)\n\n# plot the phase portrait\nxlim = (-3, 3)\nplot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)"},"outputs":[],"source":"# compute and plot the unstable periodic orbit\nlimit_cycle = VanDerPolOscillator.CalcLimitCycle()\nplt.plot(\n    limit_cycle[0],\n    limit_cycle[1],\n    color=\"b\",\n    linewidth=3,\n    label=\"ROA boundary\",\n)\nplt.legend(loc=1)\n\n# plot the phase portrait\nxlim = (-3, 3)\nplot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)"},{"block_group":"a519d312f77e4c50a551c10ae33e2f90","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"6f8270305c124fb9b4972797d8b7d8f1","deepnote_block_group":"a519d312f77e4c50a551c10ae33e2f90","deepnote_cell_type":"markdown","deepnote_sorting_key":"6","deepnote_source":"\nAs you can see, the origin of this system is locally asymptotically stable whereas, outside the Region Of Attraction (ROA) of the origin, the trajectories escape to infinity. The boundary of the ROA is an *unstable periodic orbit*: if the system state at time $t=0$ is exactly on this curve, the oscillator will orbit around the origin forever. However, any disturbance will make the system either converge to the origin or escape to infinity. Notice that the shape of this ROA is nontrivial (it is not even a convex set) and no analytic description of it is available.\n\n**Note:** Reversing the sign of $f$, we obtain the [classical Van der Pol oscillator](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator); for which the above periodic orbit is a (globally) asymptotically stable [limit cycle](https://underactuated.csail.mit.edu/simple_legs.html#limit_cycle) and the origin is an unstable equilibrium. Here we reverse time (i.e. change the sign of $f$) to make the origin a stable equilibrium.\n\n**In this notebook we will use Sums-Of-Squares (SOS) optimization to find an inner approximation of the ROA of the equilibrium point in the origin. We will write three different SOS optimizations, and analyze their pros and cons.**"},"source":"\nAs you can see, the origin of this system is locally asymptotically stable whereas, outside the Region Of Attraction (ROA) of the origin, the trajectories escape to infinity. The boundary of the ROA is an *unstable periodic orbit*: if the system state at time $t=0$ is exactly on this curve, the oscillator will orbit around the origin forever. However, any disturbance will make the system either converge to the origin or escape to infinity. Notice that the shape of this ROA is nontrivial (it is not even a convex set) and no analytic description of it is available.\n\n**Note:** Reversing the sign of $f$, we obtain the [classical Van der Pol oscillator](https://en.wikipedia.org/wiki/Van_der_Pol_oscillator); for which the above periodic orbit is a (globally) asymptotically stable [limit cycle](https://underactuated.csail.mit.edu/simple_legs.html#limit_cycle) and the origin is an unstable equilibrium. Here we reverse time (i.e. change the sign of $f$) to make the origin a stable equilibrium.\n\n**In this notebook we will use Sums-Of-Squares (SOS) optimization to find an inner approximation of the ROA of the equilibrium point in the origin. We will write three different SOS optimizations, and analyze their pros and cons.**"},{"block_group":"c023cc0fb34d41ae9f64c17fa4245ff9","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"661041474e4c473b946b61c31c5384b0","deepnote_block_group":"c023cc0fb34d41ae9f64c17fa4245ff9","deepnote_cell_type":"markdown","deepnote_sorting_key":"7","deepnote_source":"## Lyapunov Function via Linearization\nThe approach we will follow to estimate the ROA of the oscillator is the following:\n\n- We linearize the dynamics $\\dot{\\mathbf{x}} = f(\\mathbf{x})$ around the origin, to get $\\dot{\\mathbf{x}} = A \\mathbf{x}$.\n\n- We solve the Lyapunov equation $$A^T P + P A = - Q$$ to get the positive definite matrix $P$, and the Lyapunov function $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$ for the linearized system. **Note**: A standard choice for $Q$ is an identity matrix.\n\n- Lyapunov theory tells us that if $A$ is strictly stable (all its eigenvalues have strictly negative real part) then the origin is a locally asymptotically stable equilibrium point for the nonlinear system $\\dot{\\mathbf{x}} = f(\\mathbf{x})$.\nMoreover, a conservative approximation of the ROA can be obtained using the Lyapunov function $V(\\mathbf{x})$ derived in the linear analysis.\n\n- We consider the level sets $$L(\\rho) = \\{ \\mathbf{x} : V(\\mathbf{x}) \\leq \\rho \\},$$ and we look for the maximum value of $\\rho$ such that $$\\dot{V}(\\mathbf{x}) = \\frac{\\partial V}{\\partial \\mathbf{x}} f(\\mathbf{x}) = 2 \\mathbf{x}^T P f(\\mathbf{x}) < 0, \\quad \\forall \\mathbf{x} \\in L(\\rho)\\backslash \\{0\\}.$$\nIn words, we try to find the largest level set $L(\\rho)$ entirely contained the region of space where $\\dot{V}(\\mathbf{x})$ is negative.\nLyapunov theory tells us that any trajectory that starts inside such a set will eventually converge to the origin.\n\nWe start by deriving $V(\\mathbf{x})$. In the next cell, write down the matrix $A$ (obtained by linearizing $f(x)$ near the origin), the positive definite matrix $Q$, and compute $P$ using the drake function `RealContinuousLyapunovEquation`"},"source":"## Lyapunov Function via Linearization\nThe approach we will follow to estimate the ROA of the oscillator is the following:\n\n- We linearize the dynamics $\\dot{\\mathbf{x}} = f(\\mathbf{x})$ around the origin, to get $\\dot{\\mathbf{x}} = A \\mathbf{x}$.\n\n- We solve the Lyapunov equation $$A^T P + P A = - Q$$ to get the positive definite matrix $P$, and the Lyapunov function $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$ for the linearized system. **Note**: A standard choice for $Q$ is an identity matrix.\n\n- Lyapunov theory tells us that if $A$ is strictly stable (all its eigenvalues have strictly negative real part) then the origin is a locally asymptotically stable equilibrium point for the nonlinear system $\\dot{\\mathbf{x}} = f(\\mathbf{x})$.\nMoreover, a conservative approximation of the ROA can be obtained using the Lyapunov function $V(\\mathbf{x})$ derived in the linear analysis.\n\n- We consider the level sets $$L(\\rho) = \\{ \\mathbf{x} : V(\\mathbf{x}) \\leq \\rho \\},$$ and we look for the maximum value of $\\rho$ such that $$\\dot{V}(\\mathbf{x}) = \\frac{\\partial V}{\\partial \\mathbf{x}} f(\\mathbf{x}) = 2 \\mathbf{x}^T P f(\\mathbf{x}) < 0, \\quad \\forall \\mathbf{x} \\in L(\\rho)\\backslash \\{0\\}.$$\nIn words, we try to find the largest level set $L(\\rho)$ entirely contained the region of space where $\\dot{V}(\\mathbf{x})$ is negative.\nLyapunov theory tells us that any trajectory that starts inside such a set will eventually converge to the origin.\n\nWe start by deriving $V(\\mathbf{x})$. In the next cell, write down the matrix $A$ (obtained by linearizing $f(x)$ near the origin), the positive definite matrix $Q$, and compute $P$ using the drake function `RealContinuousLyapunovEquation`"},{"block_group":"39afd6cf40a446e48420d63a11e0862c","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678228174996,"execution_millis":3,"source_hash":"d33dcca9","deepnote_to_be_reexecuted":false,"cell_id":"27c3ecba2f52449b94073f2decda6270","deepnote_block_group":"39afd6cf40a446e48420d63a11e0862c","deepnote_cell_type":"code","deepnote_sorting_key":"8","deepnote_content_hash":"d33dcca9","deepnote_execution_started_at":"2023-03-07T22:29:34.996Z","deepnote_execution_finished_at":"2023-03-07T22:29:34.999Z","deepnote_source":"A = np.eye(2)  # MODIFY HERE\nQ = np.eye(2)\nP = np.eye(2)  # MODIFY HERE"},"outputs":[],"source":"A = np.eye(2)  # MODIFY HERE\nQ = np.eye(2)\nP = np.eye(2)  # MODIFY HERE"},{"block_group":"cc1d3912bb464a0dad9cfa13d853a4af","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"485ab6f52bee4441befe4f15578c1308","deepnote_block_group":"cc1d3912bb464a0dad9cfa13d853a4af","deepnote_cell_type":"markdown","deepnote_sorting_key":"9","deepnote_source":"### Manual Check\nThe advantage of working in 2D is that we can plot things!\nBefore starting with complicated optimizations, let us plot $V(\\mathbf{x})$ and $\\dot{V}(\\mathbf{x})$ to get a sense of what we are actually looking for in this analysis."},"source":"### Manual Check\nThe advantage of working in 2D is that we can plot things!\nBefore starting with complicated optimizations, let us plot $V(\\mathbf{x})$ and $\\dot{V}(\\mathbf{x})$ to get a sense of what we are actually looking for in this analysis."},{"block_group":"09e5a0aa5c3f428f9bc638fa172a9e4d","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230488956,"execution_millis":18,"source_hash":"cd1f01fc","deepnote_to_be_reexecuted":false,"cell_id":"6fbb02be18f74ad3a2bf10aa5c6a2264","deepnote_block_group":"09e5a0aa5c3f428f9bc638fa172a9e4d","deepnote_cell_type":"code","deepnote_sorting_key":"10","deepnote_content_hash":"cd1f01fc","deepnote_execution_started_at":"2023-03-07T23:08:08.956Z","deepnote_execution_finished_at":"2023-03-07T23:08:08.974Z","deepnote_source":"# function that given rho plots the boundary\n# of the the set L(rho) defined above\n\n\ndef plot_V(rho):\n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n\n    # function that evaluates V(x) at a given x\n    # (looks bad, but it must accept meshgrids)\n    eval_V = lambda x: sum(\n        sum(x[i] * x[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n    )\n\n    # contour plot with only the rho level set\n    cs = plt.contour(\n        X1,\n        X2,\n        eval_V([X1, X2]),\n        levels=[rho],\n        colors=\"r\",\n        linewidths=3,\n        zorder=3,\n    )\n\n    # misc plot settings\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    plt.gca().set_aspect(\"equal\")\n\n    # fake plot for legend\n    plt.plot(\n        0,\n        0,\n        color=\"r\",\n        linewidth=3,\n        label=r\"$\\{ \\mathbf{x} : V(\\mathbf{x}) = \\rho \\}$\",\n    )\n    plt.legend()\n\n    return cs\n\n\n# function that plots the levels sets of Vdot(x)\ndef plot_Vdot():\n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n\n    # function that evaluates Vdot(x) at a given x\n    eval_Vdot = lambda x: 2 * sum(\n        sum(x[i] * f(x)[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n    )\n\n    # contour plot with only the rho level set\n    cs = plt.contour(\n        X1,\n        X2,\n        eval_Vdot([X1, X2]),\n        colors=\"b\",\n        levels=np.linspace(-10, 40, 11),\n    )\n    plt.gca().clabel(cs, inline=1, fontsize=10)\n\n    # misc plot settings\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    plt.gca().set_aspect(\"equal\")\n\n    # fake plot for legend\n    plt.plot(0, 0, color=\"b\", label=r\"$\\dot{V}(\\mathbf{x})$\")\n    plt.legend()\n\n    return cs"},"outputs":[],"source":"# function that given rho plots the boundary\n# of the the set L(rho) defined above\n\n\ndef plot_V(rho):\n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n\n    # function that evaluates V(x) at a given x\n    # (looks bad, but it must accept meshgrids)\n    eval_V = lambda x: sum(\n        sum(x[i] * x[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n    )\n\n    # contour plot with only the rho level set\n    cs = plt.contour(\n        X1,\n        X2,\n        eval_V([X1, X2]),\n        levels=[rho],\n        colors=\"r\",\n        linewidths=3,\n        zorder=3,\n    )\n\n    # misc plot settings\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    plt.gca().set_aspect(\"equal\")\n\n    # fake plot for legend\n    plt.plot(\n        0,\n        0,\n        color=\"r\",\n        linewidth=3,\n        label=r\"$\\{ \\mathbf{x} : V(\\mathbf{x}) = \\rho \\}$\",\n    )\n    plt.legend()\n\n    return cs\n\n\n# function that plots the levels sets of Vdot(x)\ndef plot_Vdot():\n    # grid of the state space\n    x1 = np.linspace(*xlim)\n    x2 = np.linspace(*xlim)\n    X1, X2 = np.meshgrid(x1, x2)\n\n    # function that evaluates Vdot(x) at a given x\n    eval_Vdot = lambda x: 2 * sum(\n        sum(x[i] * f(x)[j] * Pij for j, Pij in enumerate(Pi)) for i, Pi in enumerate(P)\n    )\n\n    # contour plot with only the rho level set\n    cs = plt.contour(\n        X1,\n        X2,\n        eval_Vdot([X1, X2]),\n        colors=\"b\",\n        levels=np.linspace(-10, 40, 11),\n    )\n    plt.gca().clabel(cs, inline=1, fontsize=10)\n\n    # misc plot settings\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    plt.gca().set_aspect(\"equal\")\n\n    # fake plot for legend\n    plt.plot(0, 0, color=\"b\", label=r\"$\\dot{V}(\\mathbf{x})$\")\n    plt.legend()\n\n    return cs"},{"block_group":"a945460b900941589577ed7d2b7156ac","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230489945,"execution_millis":610,"source_hash":"8704eb86","deepnote_to_be_reexecuted":false,"cell_id":"3018fdd1ecee406ba02d0499f49cdbd8","deepnote_block_group":"a945460b900941589577ed7d2b7156ac","deepnote_cell_type":"code","deepnote_sorting_key":"11","deepnote_content_hash":"8704eb86","deepnote_execution_started_at":"2023-03-07T23:08:09.945Z","deepnote_execution_finished_at":"2023-03-07T23:08:10.555Z","deepnote_source":"# tune rho by hand to make it as big as possible\n# while staying in the region where Vdot(x) is negative\nrho_max = 2.3\n\n# plot Vdot(x) and V(x) = rho\nVdot_cs = plot_Vdot()\nV_cs = plot_V(rho_max)"},"outputs":[],"source":"# tune rho by hand to make it as big as possible\n# while staying in the region where Vdot(x) is negative\nrho_max = 2.3\n\n# plot Vdot(x) and V(x) = rho\nVdot_cs = plot_Vdot()\nV_cs = plot_V(rho_max)"},{"block_group":"653cef441e894b5fb4aa7934dce91e85","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"77ef659e044449e39e80b713852c9ca3","deepnote_block_group":"653cef441e894b5fb4aa7934dce91e85","deepnote_cell_type":"markdown","deepnote_sorting_key":"12","deepnote_source":"With correct values of $A$ and $P$, you will see that the largest $\\rho$ for which $\\dot{V}(\\mathbf{x}) < 0$ is $\\approx 2.3$. The red elliptical curve (that represents the 2.3 level set for $V(\\mathbf{x})$) touches the zero-valued contours for $\\dot{V}(\\mathbf{x})$. \nSuperimposing the level set to the phase portrait, we see that this is a pretty good approximation of the ROA. "},"source":"With correct values of $A$ and $P$, you will see that the largest $\\rho$ for which $\\dot{V}(\\mathbf{x}) < 0$ is $\\approx 2.3$. The red elliptical curve (that represents the 2.3 level set for $V(\\mathbf{x})$) touches the zero-valued contours for $\\dot{V}(\\mathbf{x})$. \nSuperimposing the level set to the phase portrait, we see that this is a pretty good approximation of the ROA. "},{"block_group":"179c3b8010a947ebb2d34ae2f28cab21","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230710005,"execution_millis":439,"source_hash":"8db6c40c","deepnote_to_be_reexecuted":false,"cell_id":"1f193559f5b048c0b836e195f6f96963","deepnote_block_group":"179c3b8010a947ebb2d34ae2f28cab21","deepnote_cell_type":"code","deepnote_sorting_key":"13","deepnote_content_hash":"8db6c40c","deepnote_execution_started_at":"2023-03-07T23:11:50.005Z","deepnote_execution_finished_at":"2023-03-07T23:11:50.444Z","deepnote_source":"plot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)\nplot_V(rho_max)\nplt.plot(\n    limit_cycle[0],\n    limit_cycle[1],\n    color=\"b\",\n    linewidth=3,\n    label=\"ROA boundary\",\n)\nplt.legend(loc=1)"},"outputs":[],"source":"plot_2d_phase_portrait(f, x1lim=xlim, x2lim=xlim)\nplot_V(rho_max)\nplt.plot(\n    limit_cycle[0],\n    limit_cycle[1],\n    color=\"b\",\n    linewidth=3,\n    label=\"ROA boundary\",\n)\nplt.legend(loc=1)"},{"block_group":"5a8658e82e2449acbaa694c4ac33f84d","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"ee621ec9dc1a4b469bfc75460875bd49","deepnote_block_group":"5a8658e82e2449acbaa694c4ac33f84d","deepnote_cell_type":"markdown","deepnote_sorting_key":"14","deepnote_source":"Of course, when $\\text{dim}(\\mathbf{x}) > 2$, a \"manual\" maximization of $\\rho$ has no hope to work.\nThat's where SOS programming really makes the difference!\nThe goal of this notebook is to experiment these tools on a case where things can actually be visualized, so that we get a better sense of the power of this technique."},"source":"Of course, when $\\text{dim}(\\mathbf{x}) > 2$, a \"manual\" maximization of $\\rho$ has no hope to work.\nThat's where SOS programming really makes the difference!\nThe goal of this notebook is to experiment these tools on a case where things can actually be visualized, so that we get a better sense of the power of this technique."},{"block_group":"62de3e11973e442b844c583297a990ee","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"e88e11ae3f1b49d9ba430e92815c8100","deepnote_block_group":"62de3e11973e442b844c583297a990ee","deepnote_cell_type":"markdown","deepnote_sorting_key":"15","deepnote_source":"## Method 1: Line-Search on $\\rho$\nThe first method we use to estimate the ROA is the one from the textbook example \"[Region of attraction for the one-dimensional cubic system](https://underactuated.csail.mit.edu/lyapunov.html#roa_cubic_system)\".\n\nWe look for the largest $\\rho$ for which there exists a SOS polynomial $\\lambda(\\mathbf{x})$ such that $$- \\dot{V}(\\mathbf{x}) - \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) - \\epsilon \\mathbf{x}^T \\mathbf{x} \\ \\text{is SOS},$$\nwith $\\epsilon$ very small.\nThis problem cannot be written as a single SOS program, since both the polynomial $\\lambda$ and scalar $\\rho$ are decision variables, which are here multiplied together. Hence we naively solve a sequence of SOS programs with increasing value of $\\rho$.\n\nThe intuition behind this formulation is the following.\nThink of the condition \"is SOS\" as \"$\\geq 0$\" (actually, the first is sufficient for the second).\nThen what we are asking is $- \\dot{V}(\\mathbf{x}) \\geq \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) + \\epsilon \\mathbf{x}^T \\mathbf{x}$. \nInside the level set $L(\\rho)$, we have $\\rho - V(\\mathbf{x}) \\geq 0$ and, since $\\lambda(\\mathbf{x})$ is SOS, $\\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) \\geq 0$.\nThus the condition above is just sayng that, for all $\\mathbf{x}$ in $L(\\rho)$, we must have $- \\dot{V}(\\mathbf{x})\\geq \\epsilon \\mathbf{x}^T \\mathbf{x}$, i.e., $\\dot{V}(\\mathbf{x})$ negative definite."},"source":"## Method 1: Line-Search on $\\rho$\nThe first method we use to estimate the ROA is the one from the textbook example \"[Region of attraction for the one-dimensional cubic system](https://underactuated.csail.mit.edu/lyapunov.html#roa_cubic_system)\".\n\nWe look for the largest $\\rho$ for which there exists a SOS polynomial $\\lambda(\\mathbf{x})$ such that $$- \\dot{V}(\\mathbf{x}) - \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) - \\epsilon \\mathbf{x}^T \\mathbf{x} \\ \\text{is SOS},$$\nwith $\\epsilon$ very small.\nThis problem cannot be written as a single SOS program, since both the polynomial $\\lambda$ and scalar $\\rho$ are decision variables, which are here multiplied together. Hence we naively solve a sequence of SOS programs with increasing value of $\\rho$.\n\nThe intuition behind this formulation is the following.\nThink of the condition \"is SOS\" as \"$\\geq 0$\" (actually, the first is sufficient for the second).\nThen what we are asking is $- \\dot{V}(\\mathbf{x}) \\geq \\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) + \\epsilon \\mathbf{x}^T \\mathbf{x}$. \nInside the level set $L(\\rho)$, we have $\\rho - V(\\mathbf{x}) \\geq 0$ and, since $\\lambda(\\mathbf{x})$ is SOS, $\\lambda(\\mathbf{x}) (\\rho - V(\\mathbf{x})) \\geq 0$.\nThus the condition above is just sayng that, for all $\\mathbf{x}$ in $L(\\rho)$, we must have $- \\dot{V}(\\mathbf{x})\\geq \\epsilon \\mathbf{x}^T \\mathbf{x}$, i.e., $\\dot{V}(\\mathbf{x})$ negative definite."},{"block_group":"391f396319db458cb6a20feda8325610","cell_type":"markdown","execution_count":null,"metadata":{"tags":[],"cell_id":"09b66045b4534f8390bf1c5da1d2426a","deepnote_block_group":"391f396319db458cb6a20feda8325610","deepnote_cell_type":"markdown","deepnote_sorting_key":"16","deepnote_source":"Carefully read the following code, as later you will be asked to write similar mathmatical programs. The following drake [tutorial on SOS](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) can be helpful."},"source":"Carefully read the following code, as later you will be asked to write similar mathmatical programs. The following drake [tutorial on SOS](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) can be helpful."},{"block_group":"29610a47696146c597244af8c8f7c407","cell_type":"code","execution_count":null,"metadata":{"allow_embed":false,"execution_start":1678230719391,"execution_millis":3,"source_hash":"81bfd7c6","deepnote_to_be_reexecuted":false,"cell_id":"de1642c35c0e43c0bcac41f9ad2b3450","deepnote_block_group":"29610a47696146c597244af8c8f7c407","deepnote_cell_type":"code","deepnote_sorting_key":"17","deepnote_content_hash":"81bfd7c6","deepnote_execution_started_at":"2023-03-07T23:11:59.391Z","deepnote_execution_finished_at":"2023-03-07T23:11:59.394Z","deepnote_source":"# function that verifies the condition described above\n# for the level set L(rho) for a given rho\n\n\ndef is_verified(rho):\n    # initialize optimization problem\n    # (with Drake there is no need to specify that\n    # this is going to be a SOS program!)\n    prog = MathematicalProgram()\n\n    # SOS indeterminates\n    x = prog.NewIndeterminates(2, \"x\")\n\n    # Lyapunov function\n    V = x.dot(P).dot(x)\n    V_dot = 2 * x.dot(P).dot(f(x))\n\n    # degree of the polynomial lambda(x)\n    # no need to change it, but if you really want to,\n    # keep l_deg even (why?) and do not set l_deg greater than 10\n    # (otherwise optimizations will take forever)\n    l_deg = 4\n    assert l_deg % 2 == 0\n\n    # SOS Lagrange multipliers\n    l = prog.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()\n\n    # main condition above\n    eps = 1e-3  # do not change\n    prog.AddSosConstraint(-V_dot - l * (rho - V) - eps * x.dot(x))\n\n    # solve SOS program\n    # no objective function in this formulation\n    result = Solve(prog)\n\n    # return True if feasible, False if infeasible\n    return result.is_success()"},"outputs":[],"source":"# function that verifies the condition described above\n# for the level set L(rho) for a given rho\n\n\ndef is_verified(rho):\n    # initialize optimization problem\n    # (with Drake there is no need to specify that\n    # this is going to be a SOS program!)\n    prog = MathematicalProgram()\n\n    # SOS indeterminates\n    x = prog.NewIndeterminates(2, \"x\")\n\n    # Lyapunov function\n    V = x.dot(P).dot(x)\n    V_dot = 2 * x.dot(P).dot(f(x))\n\n    # degree of the polynomial lambda(x)\n    # no need to change it, but if you really want to,\n    # keep l_deg even (why?) and do not set l_deg greater than 10\n    # (otherwise optimizations will take forever)\n    l_deg = 4\n    assert l_deg % 2 == 0\n\n    # SOS Lagrange multipliers\n    l = prog.NewSosPolynomial(Variables(x), l_deg)[0].ToExpression()\n\n    # main condition above\n    eps = 1e-3  # do not change\n    prog.AddSosConstraint(-V_dot - l * (rho - V) - eps * x.dot(x))\n\n    # solve SOS program\n    # no objective function in this formulation\n    result = Solve(prog)\n\n    # return True if feasible, False if infeasible\n    return result.is_success()"},{"block_group":"011e63360be64b869fb1411cfd189e50","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"5152e5fc56c348eab585b2d305bba2d4","deepnote_block_group":"011e63360be64b869fb1411cfd189e50","deepnote_cell_type":"markdown","deepnote_sorting_key":"18","deepnote_source":"Now that we have the building block of our algorithm, it's your time to write the line search to find the maximum $\\rho$ for which the condition described above holds.\n\nImplement the line search in next cell.\nStart with `rho = 0`, check if the level set $L(\\rho)$ is verified (i.e. the function `is_verified(rho)` returns `True`); if yes, increase `rho` by `rho_step = .01`, if no, assign to the variable `rho_method_1` the maximum verified `rho` you've found with this procedure."},"source":"Now that we have the building block of our algorithm, it's your time to write the line search to find the maximum $\\rho$ for which the condition described above holds.\n\nImplement the line search in next cell.\nStart with `rho = 0`, check if the level set $L(\\rho)$ is verified (i.e. the function `is_verified(rho)` returns `True`); if yes, increase `rho` by `rho_step = .01`, if no, assign to the variable `rho_method_1` the maximum verified `rho` you've found with this procedure."},{"block_group":"51daeaa64dd24e9a96fe2926cc8ccc27","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230727013,"execution_millis":2,"source_hash":"e7b94275","deepnote_to_be_reexecuted":false,"cell_id":"0cb2682c3fb140ceb8be5699ad964aa6","deepnote_block_group":"51daeaa64dd24e9a96fe2926cc8ccc27","deepnote_cell_type":"code","deepnote_sorting_key":"19","deepnote_content_hash":"e7b94275","deepnote_execution_started_at":"2023-03-07T23:12:07.013Z","deepnote_execution_finished_at":"2023-03-07T23:12:07.015Z","deepnote_source":"# implement your line-search here\n\n\n# set the maximum value of rho you've found with line search\nrho_method_1 = 0  # MODIFY HERE\n\nprint(f\"Method 1 verified rho = {rho_method_1}.\")"},"outputs":[],"source":"# implement your line-search here\n\n\n# set the maximum value of rho you've found with line search\nrho_method_1 = 0  # MODIFY HERE\n\nprint(f\"Method 1 verified rho = {rho_method_1}.\")"},{"block_group":"16ce3bb868744a77b7398568e4c92a0f","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"913bd010a47e4804bcfca53e6537e046","deepnote_block_group":"16ce3bb868744a77b7398568e4c92a0f","deepnote_cell_type":"markdown","deepnote_sorting_key":"20","deepnote_source":"Did this method do a good job in approximating (from below) the maximum $\\rho$ we have found by hand?"},"source":"Did this method do a good job in approximating (from below) the maximum $\\rho$ we have found by hand?"},{"block_group":"a6dbbcb083ef41dc95949bf41086f4e0","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"5c48574202eb4006b18e3bf4ad32749d","deepnote_block_group":"a6dbbcb083ef41dc95949bf41086f4e0","deepnote_cell_type":"markdown","deepnote_sorting_key":"21","deepnote_source":"## Method 2: Single-Shot SOS Program\nWith the previous formulation we had to solve a sequence of SOS programs, now we consider an equivalent formulation of the SOS problem in which we can directly maximize $\\rho$.\nIn the previous case we wrote a SOS program to check the implication\n$$\\mathbf{x} \\in L(\\rho) \\Rightarrow \\dot{V}(\\mathbf{x}) < 0.$$\nThis, however, can be equivalently stated as\n$$\\dot{V}(\\mathbf{x}) \\geq 0 \\Rightarrow \\mathbf{x} \\not\\in L(\\rho).$$\nExpressing $\\mathbf{x} \\not\\in L(\\rho)$ as $V(\\mathbf{x}) - \\rho \\geq 0$, it turns out that the latter condition can be verified with a single SOS program.\n(Actually, we should say $V(\\mathbf{x}) - \\rho > 0$, but working with a computer there is no difference.)\n\nThe new problem reads as follows.\nFind an SOS polynomial $\\lambda(\\mathbf{x})$ such that\n$$V(\\mathbf{x}) - \\rho - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS}.$$\nIn fact, this implies $V(\\mathbf{x}) - \\rho \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$ and, for all $\\mathbf{x}$ where $\\dot{V}(\\mathbf{x}) \\geq 0$, we get $V(\\mathbf{x}) - \\rho \\geq 0$.\n\nNotice that now $\\lambda$ does not multiply $\\rho$, and we can search over both of them at the same time.\nHence we can ask the optimizer to maximize $\\rho$.\nThere is however an issue with the current problem formulation...\n\n### Not quite there yet...\n\nDo you see anything wrong with the problem formulation we put together so far? What do you think the maximum $\\rho$ will be?\n\nAs stated so far, the problem will always return $\\rho = 0$!\nTo see why, first notice that for $\\rho = \\lambda = 0$ the SOS condition above would become $V(\\mathbf{x})$ is SOS, which holds since $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$.\nNow consider a positive $\\rho$.\nSince $V(0) = \\dot{V} (0) = 0$, evaluating the SOS condition in the origin, we would get $-\\rho \\geq 0$ which can never hold!\n\nNot everything is lost, we have a neat fix for you.\nCertainly, if $V(\\mathbf{x}) - \\rho$ is nonnegative, so is $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho)$.\nHence we consider the SOS condition\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nwith $\\lambda(\\mathbf{x})$ SOS.\nNow the issue in the origin is fixed, since for $\\mathbf{x} = 0$, we get \"$0$ is SOS\", which is always true.\nMoreover, where $\\mathbf{x}$ is such that $\\dot{V}(\\mathbf{x}) \\geq 0$, the new SOS condition requires $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$ and hence $V(\\mathbf{x}) - \\rho \\geq 0$ as desired.\n\nNow we are good to go!"},"source":"## Method 2: Single-Shot SOS Program\nWith the previous formulation we had to solve a sequence of SOS programs, now we consider an equivalent formulation of the SOS problem in which we can directly maximize $\\rho$.\nIn the previous case we wrote a SOS program to check the implication\n$$\\mathbf{x} \\in L(\\rho) \\Rightarrow \\dot{V}(\\mathbf{x}) < 0.$$\nThis, however, can be equivalently stated as\n$$\\dot{V}(\\mathbf{x}) \\geq 0 \\Rightarrow \\mathbf{x} \\not\\in L(\\rho).$$\nExpressing $\\mathbf{x} \\not\\in L(\\rho)$ as $V(\\mathbf{x}) - \\rho \\geq 0$, it turns out that the latter condition can be verified with a single SOS program.\n(Actually, we should say $V(\\mathbf{x}) - \\rho > 0$, but working with a computer there is no difference.)\n\nThe new problem reads as follows.\nFind an SOS polynomial $\\lambda(\\mathbf{x})$ such that\n$$V(\\mathbf{x}) - \\rho - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS}.$$\nIn fact, this implies $V(\\mathbf{x}) - \\rho \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$ and, for all $\\mathbf{x}$ where $\\dot{V}(\\mathbf{x}) \\geq 0$, we get $V(\\mathbf{x}) - \\rho \\geq 0$.\n\nNotice that now $\\lambda$ does not multiply $\\rho$, and we can search over both of them at the same time.\nHence we can ask the optimizer to maximize $\\rho$.\nThere is however an issue with the current problem formulation...\n\n### Not quite there yet...\n\nDo you see anything wrong with the problem formulation we put together so far? What do you think the maximum $\\rho$ will be?\n\nAs stated so far, the problem will always return $\\rho = 0$!\nTo see why, first notice that for $\\rho = \\lambda = 0$ the SOS condition above would become $V(\\mathbf{x})$ is SOS, which holds since $V(\\mathbf{x}) = \\mathbf{x}^T P \\mathbf{x}$.\nNow consider a positive $\\rho$.\nSince $V(0) = \\dot{V} (0) = 0$, evaluating the SOS condition in the origin, we would get $-\\rho \\geq 0$ which can never hold!\n\nNot everything is lost, we have a neat fix for you.\nCertainly, if $V(\\mathbf{x}) - \\rho$ is nonnegative, so is $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho)$.\nHence we consider the SOS condition\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nwith $\\lambda(\\mathbf{x})$ SOS.\nNow the issue in the origin is fixed, since for $\\mathbf{x} = 0$, we get \"$0$ is SOS\", which is always true.\nMoreover, where $\\mathbf{x}$ is such that $\\dot{V}(\\mathbf{x}) \\geq 0$, the new SOS condition requires $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$ and hence $V(\\mathbf{x}) - \\rho \\geq 0$ as desired.\n\nNow we are good to go!"},{"block_group":"f06b90d969294a21a68c64e423bba4dc","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"813dadc512e9425bb5d0e30abb2c10d3","deepnote_block_group":"f06b90d969294a21a68c64e423bba4dc","deepnote_cell_type":"markdown","deepnote_sorting_key":"22","deepnote_source":"In the next cell you need to code the SOS program we just described. Refer to the previous code block and the [drake tutorial](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) for hint on writing SOS programs. "},"source":"In the next cell you need to code the SOS program we just described. Refer to the previous code block and the [drake tutorial](https://deepnote.com/workspace/Drake-0b3b2c53-a7ad-441b-80f8-bf8350752305/project/Tutorials-2b4fc509-aef2-417d-a40d-6071dfed9199/notebook/sum_of_squares_optimization-4137868b87ee423dbc50d0076f72bbb9) for hint on writing SOS programs. "},{"block_group":"83557c28bdca4004a970d5bd7aeaf33c","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230730517,"execution_millis":2,"source_hash":"ea4a7205","deepnote_to_be_reexecuted":false,"cell_id":"cff2d04f43704ac5bf0fc794c9602083","deepnote_block_group":"83557c28bdca4004a970d5bd7aeaf33c","deepnote_cell_type":"code","deepnote_sorting_key":"23","deepnote_content_hash":"ea4a7205","deepnote_execution_started_at":"2023-03-07T23:12:10.517Z","deepnote_execution_finished_at":"2023-03-07T23:12:10.519Z","deepnote_source":"# initialize optimization problem\nprog2 = MathematicalProgram()  # Do not modify\n\n# 1. define SOS indeterminates\n\n# 2. define Lyapunov function and its derivative\n\n# 3. define SOS Lagrange multipliers, with any even degree between 4 and 10\n\n# 4. define 'rho' (level set) as optimization variable\n\n# 5. write the SOS constraints as discussed above\n\n# 6. write the objective function (use AddLinearCost)\n\n# 7. call the solver on the mathematical program, and set the value of rho_method_2 below.\n\nrho_method_2 = 0\n\nprint(f\"Method 2 verified rho = {rho_method_2}.\")"},"outputs":[],"source":"# initialize optimization problem\nprog2 = MathematicalProgram()  # Do not modify\n\n# 1. define SOS indeterminates\n\n# 2. define Lyapunov function and its derivative\n\n# 3. define SOS Lagrange multipliers, with any even degree between 4 and 10\n\n# 4. define 'rho' (level set) as optimization variable\n\n# 5. write the SOS constraints as discussed above\n\n# 6. write the objective function (use AddLinearCost)\n\n# 7. call the solver on the mathematical program, and set the value of rho_method_2 below.\n\nrho_method_2 = 0\n\nprint(f\"Method 2 verified rho = {rho_method_2}.\")"},{"block_group":"bdc594219a244ab28a6f89ecc6b5be75","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"33c5f3d1da1c4c3184a77c0ba441a7da","deepnote_block_group":"bdc594219a244ab28a6f89ecc6b5be75","deepnote_cell_type":"markdown","deepnote_sorting_key":"24","deepnote_source":"## Method 3: Smarter Single-Shot SOS Program\nThe SOS program we just wrote was already a satisfying solution, but it turns out we can do even better!\nFrom the textbook chapter \"[Lyapunov analysis with convex optimization](https://underactuated.csail.mit.edu/lyapunov.html#optimization)\", you know that every SOS constraint brings with it a lot of optimization variables and an SDP constraint.\nSo, whenever we can, removing redundant SOS requirements is always a good thing to do.\n\nWe claim that in the previous formulation we don't need $\\lambda(\\mathbf{x})$ to be SOS. How is this possible?\n\nWe start by noticing that, in a neighborhood of the origin, excluded the origin itself, $\\dot{V}(\\mathbf{x})$ is strictly negative.\n(This because $V(\\mathbf{x})$ is a Lyapunov function for the linearized system hence, locally, it works also for the nonlinear system.)\n\nIn light of the latter observation, instead of asking that $\\dot{V}(\\mathbf{x})$ is negative for all $\\mathbf{x} \\neq 0$ in $L(\\rho)$, we can equivalently ask that all the points $\\mathbf{x} \\neq 0$ where $\\dot{V}(\\mathbf{x}) = 0$ must be outside the level set $L(\\rho)$.\nThis might take a second to parse!\n\nThe latter condition can be enforced exactly as the one above:\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nbut this time we do not require $\\lambda(\\mathbf{x})$ to be SOS.\n\nHere is the reasoning.\nFirst, notice that this condition implies $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$.\nThen, observe that for all $\\mathbf{x}$ such that $\\dot{V}(\\mathbf{x}) = 0$, we get $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$.\nThis implies $V(\\mathbf{x}) - \\rho \\geq 0$, i.e., $\\mathbf{x} \\not\\in L(\\rho)$ as desired.\n(As before, no need to care about what happens at the boundary of the level set.)\n\nThis trick can make a huge difference when you need to verify high-dimensional systems!"},"source":"## Method 3: Smarter Single-Shot SOS Program\nThe SOS program we just wrote was already a satisfying solution, but it turns out we can do even better!\nFrom the textbook chapter \"[Lyapunov analysis with convex optimization](https://underactuated.csail.mit.edu/lyapunov.html#optimization)\", you know that every SOS constraint brings with it a lot of optimization variables and an SDP constraint.\nSo, whenever we can, removing redundant SOS requirements is always a good thing to do.\n\nWe claim that in the previous formulation we don't need $\\lambda(\\mathbf{x})$ to be SOS. How is this possible?\n\nWe start by noticing that, in a neighborhood of the origin, excluded the origin itself, $\\dot{V}(\\mathbf{x})$ is strictly negative.\n(This because $V(\\mathbf{x})$ is a Lyapunov function for the linearized system hence, locally, it works also for the nonlinear system.)\n\nIn light of the latter observation, instead of asking that $\\dot{V}(\\mathbf{x})$ is negative for all $\\mathbf{x} \\neq 0$ in $L(\\rho)$, we can equivalently ask that all the points $\\mathbf{x} \\neq 0$ where $\\dot{V}(\\mathbf{x}) = 0$ must be outside the level set $L(\\rho)$.\nThis might take a second to parse!\n\nThe latter condition can be enforced exactly as the one above:\n$$\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) - \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x}) \\ \\text{is SOS},$$\nbut this time we do not require $\\lambda(\\mathbf{x})$ to be SOS.\n\nHere is the reasoning.\nFirst, notice that this condition implies $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq \\lambda(\\mathbf{x}) \\dot{V}(\\mathbf{x})$.\nThen, observe that for all $\\mathbf{x}$ such that $\\dot{V}(\\mathbf{x}) = 0$, we get $\\mathbf{x}^T\\mathbf{x}(V(\\mathbf{x}) - \\rho) \\geq 0$.\nThis implies $V(\\mathbf{x}) - \\rho \\geq 0$, i.e., $\\mathbf{x} \\not\\in L(\\rho)$ as desired.\n(As before, no need to care about what happens at the boundary of the level set.)\n\nThis trick can make a huge difference when you need to verify high-dimensional systems!"},{"block_group":"330d739ccc574e07bd17364a6daac680","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"31480da396194857b0769c76feda77ce","deepnote_block_group":"330d739ccc574e07bd17364a6daac680","deepnote_cell_type":"markdown","deepnote_sorting_key":"25","deepnote_source":"To try this new idea, in the next cell define the third mathematical program that has only one SOS constraint. **Note:** Do not use `NewSosPolynomial` for defining $\\lambda(x)$, instead use `NewFreePolynomial`. If you have done thing correctly, `rho_method_3` should \"closely match\" `rho_method_2`!"},"source":"To try this new idea, in the next cell define the third mathematical program that has only one SOS constraint. **Note:** Do not use `NewSosPolynomial` for defining $\\lambda(x)$, instead use `NewFreePolynomial`. If you have done thing correctly, `rho_method_3` should \"closely match\" `rho_method_2`!"},{"block_group":"fc903f5dd0454d3b899f0b5a307ac51b","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230733277,"execution_millis":2,"source_hash":"759c025e","deepnote_to_be_reexecuted":false,"cell_id":"81bbccc29dde4546a65ba2c5a6d5f0f1","deepnote_block_group":"fc903f5dd0454d3b899f0b5a307ac51b","deepnote_cell_type":"code","deepnote_sorting_key":"26","deepnote_content_hash":"759c025e","deepnote_execution_started_at":"2023-03-07T23:12:13.277Z","deepnote_execution_finished_at":"2023-03-07T23:12:13.279Z","deepnote_source":"# initialize optimization problem\nprog3 = MathematicalProgram()  # Do not modify\n\n# Write the modified Single-Shot SOS program here. Make sure there is only one SOS constraint.\n\nrho_method_3 = 0  # MODIFY HERE\n\nprint(f\"Method 3 verified rho = {rho_method_3}.\")"},"outputs":[],"source":"# initialize optimization problem\nprog3 = MathematicalProgram()  # Do not modify\n\n# Write the modified Single-Shot SOS program here. Make sure there is only one SOS constraint.\n\nrho_method_3 = 0  # MODIFY HERE\n\nprint(f\"Method 3 verified rho = {rho_method_3}.\")"},{"block_group":"581c8d5946c94ed6a62dc53ae3f9902f","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"a8e4f0c595da4c669976d8af996f5e9e","deepnote_block_group":"581c8d5946c94ed6a62dc53ae3f9902f","deepnote_cell_type":"markdown","deepnote_sorting_key":"27","deepnote_source":"## Final Note\nMore advanced techniques to approximate ROAs are available.\nGenerally they require some sort of alternation between optimizing over the multiplier $\\lambda$ (as we did here) and modifying the shape of the Lyapunov function $V(\\mathbf{x})$, e.g., by considering higher-order polynomials (here we stuck to the quadratic one coming from\nlinear analysis).\nThe level set $\\rho$ is generally kept fixed (e.g. equal to unity) since, when reshaping the Lyapunov function, the optimizer is allowed to scale the range of this function arbitrarily.\n\nHere is an image of SOS in its full glory approximating the ROA of the Van der Pol oscillator.\nImpressive, isn't it?!\n\n![figure](https://raw.githubusercontent.com/RussTedrake/underactuated/master/book/figures/exercises/van_der_pol_roa.png)\n(Courtesy of Shen Shen.)"},"source":"## Final Note\nMore advanced techniques to approximate ROAs are available.\nGenerally they require some sort of alternation between optimizing over the multiplier $\\lambda$ (as we did here) and modifying the shape of the Lyapunov function $V(\\mathbf{x})$, e.g., by considering higher-order polynomials (here we stuck to the quadratic one coming from\nlinear analysis).\nThe level set $\\rho$ is generally kept fixed (e.g. equal to unity) since, when reshaping the Lyapunov function, the optimizer is allowed to scale the range of this function arbitrarily.\n\nHere is an image of SOS in its full glory approximating the ROA of the Van der Pol oscillator.\nImpressive, isn't it?!\n\n![figure](https://raw.githubusercontent.com/RussTedrake/underactuated/master/book/figures/exercises/van_der_pol_roa.png)\n(Courtesy of Shen Shen.)"},{"block_group":"de7705722adc449ca907e298a722e61c","cell_type":"markdown","execution_count":null,"metadata":{"cell_id":"6461fa7941b14fc1a9570c7e70056a31","deepnote_block_group":"de7705722adc449ca907e298a722e61c","deepnote_cell_type":"markdown","deepnote_sorting_key":"28","deepnote_source":"## Autograding\nYou can check your work by running the following cell:"},"source":"## Autograding\nYou can check your work by running the following cell:"},{"block_group":"6186447f664648818b01d86f894604b2","cell_type":"code","execution_count":null,"metadata":{"execution_start":1678230738080,"execution_millis":4,"source_hash":"667bdfaf","deepnote_to_be_reexecuted":false,"cell_id":"7e9c662be198431bbabcd2dfb81eb93e","deepnote_block_group":"6186447f664648818b01d86f894604b2","deepnote_cell_type":"code","deepnote_sorting_key":"29","deepnote_content_hash":"667bdfaf","deepnote_execution_started_at":"2023-03-07T23:12:18.080Z","deepnote_execution_finished_at":"2023-03-07T23:12:18.084Z","deepnote_source":"from underactuated.exercises.grader import Grader\nfrom underactuated.exercises.lyapunov.test_van_der_pol import TestVanDerPol\n\nGrader.grade_output([TestVanDerPol], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"},"outputs":[],"source":"from underactuated.exercises.grader import Grader\nfrom underactuated.exercises.lyapunov.test_van_der_pol import TestVanDerPol\n\nGrader.grade_output([TestVanDerPol], [locals()], \"results.json\")\nGrader.print_test_results(\"results.json\")"}],
        "metadata": {"deepnote_notebook_id":"d5f3e1068e3a43949eb00cd7d778b99d"},
        "nbformat": "4",
        "nbformat_minor": "0",
        "version": "0"
      }